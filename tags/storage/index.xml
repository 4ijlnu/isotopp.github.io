<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>storage on Die wunderbare Welt von Isotopp</title>
    <link>https://blog.koehntopp.info/tags/storage.html</link>
    <description>Recent content in storage on Die wunderbare Welt von Isotopp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.koehntopp.info/tags/storage/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NVME is not a hard disk</title>
      <link>https://blog.koehntopp.info/2021/05/25/nvme-is-not-a-hard-disk.html</link>
      <pubDate>Tue, 25 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2021/05/25/nvme-is-not-a-hard-disk.html</guid>
      <description>So somebody tweeted about the Seagate Mach.2 , a harddisk with two independent heads &amp;ldquo;combs&amp;rdquo;, and I commented in german : &amp;ldquo;It&amp;rsquo;s two drives in one chassis, even shown as two drives. And it still is rotating rust, so slow with seeks. Linear IO will be fine.&amp;rdquo;
That quickly devolved in a discussion of RAID-0 on a single disk drive : &amp;ldquo;RAID-0 on a single physical drive. Yeah, you can do that if you do not need your data.</description>
    </item>
    
    <item>
      <title>Trying lvmraid for real</title>
      <link>https://blog.koehntopp.info/2019/12/03/trying-lvmraid-for-real.html</link>
      <pubDate>Tue, 03 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2019/12/03/trying-lvmraid-for-real.html</guid>
      <description>So after testing LVM Raid in princple, I have been trying it on some real hardware to see what happens. The idea was to estimate if it scales and if not, how it doesn&amp;rsquo;t. I was expecting to run into all kinds of obscure problems in my testing, but in fact, it was a quick and short death.
Here is my box: QuantaGrid D42A-2U with an AMD EPYC 7551P CPU (32C, HT off), 1024 GB of memory, a boot disk and 12x Micron_9200_MTFDHAL11TATCW (PDF ) for 120TB of disk storage.</description>
    </item>
    
    <item>
      <title>Cloning and splitting logical volumes</title>
      <link>https://blog.koehntopp.info/2019/12/02/cloning-and-splitting-logical-volumes.html</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2019/12/02/cloning-and-splitting-logical-volumes.html</guid>
      <description>Where I work, we routinely run our databases on XFS on LVM2.
The setup   Each database has their database on /mysql/schemaname, with the subdirectories /mysql/schemaname/{data,log,tmp}. The entire /mysql/schemname tree is a LVM2 Logical Volume mysqlVol on the Volume Group vg00, which is then formatted as an XFS filesystem.
# pvcreate /dev/nvme*n1 # vgcreate vg00 /dev/nvme*n1 # lvcreate -n mysqlVol -L...G vg00 # mkfs -t xfs /dev/vg00/mysqlVol # mount -t xfs /dev/vg00/mysqlVol /mysql/schemaname Basic Ops   You can grow an existing LVM Logical Volume with lvextend -L+50G /dev/vg00/mysqlVol or similar, and then xfs_grow /dev/vg00/myqlVol.</description>
    </item>
    
    <item>
      <title>Filling disk space fast</title>
      <link>https://blog.koehntopp.info/2019/11/11/filling-disk-space-fast.html</link>
      <pubDate>Mon, 11 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2019/11/11/filling-disk-space-fast.html</guid>
      <description>Some of the databases at work are a tad on the large side, in the high 2-digit terabytes of size. Copying these to new machines at the moment takes a rather long time, multiple days, up to a week. Speeding it up pays twice, because with shorter copy times there is also less binlog to catch up.
I have been looking into disk copy speeds in order to better understand the limits.</description>
    </item>
    
    <item>
      <title>Ceph and NVME - not a good combination?</title>
      <link>https://blog.koehntopp.info/2019/07/16/ceph-and-nvme-not-a-good-combination.html</link>
      <pubDate>Tue, 16 Jul 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2019/07/16/ceph-and-nvme-not-a-good-combination.html</guid>
      <description>Saving an older twitter thread on Ceph .
 I just have finished reading Part 4: RHCS 3.2 Bluestore Advanced Performance Investigation and now I do not know what to say.
Ceph set out with the idea to make storage a commodity by using regular PCs + a lot of local hard disks + software to make inexpensive storage. For that, you ran a number of control processes and an OSD process per disk (&amp;ldquo;object storage demon&amp;rdquo;).</description>
    </item>
    
    <item>
      <title>Not quite storage any more</title>
      <link>https://blog.koehntopp.info/2019/06/14/not-quite-storage-any-more.html</link>
      <pubDate>Fri, 14 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2019/06/14/not-quite-storage-any-more.html</guid>
      <description>While I was testing away on all the SSD and NVME toys, I got my hand on a test box with even stranger that usual equipment. It was a Dual Platinum-8280 box with a really weird amount of memory: 7.5 TB.
“8280 ”. This is a machine with a “2” in the second digit, indicating Cascade Lake , CLX.
And one thing that is new with CLX is “Intel Optane Persistent Memory Technology”.</description>
    </item>
    
    <item>
      <title>Adventures in Storageland</title>
      <link>https://blog.koehntopp.info/2019/06/13/adventures-in-storageland.html</link>
      <pubDate>Thu, 13 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2019/06/13/adventures-in-storageland.html</guid>
      <description>Adventures in Storageland   Devices   In the last few weeks, I have been benchmarking server storage in order to get a better idea what the state of the hardware is these days. There is a summary with recommendations at the end of this article. Storage technology
In the past, we had stored data on rotating disks.
An open hard disk chassis. You can see the topmost of a stack of disks, and the arm with the r/w heads.</description>
    </item>
    
  </channel>
</rss>
