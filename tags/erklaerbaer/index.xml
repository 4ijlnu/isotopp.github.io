<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>erklaerbaer on Die wunderbare Welt von Isotopp</title>
    <link>/tags/erklaerbaer.html</link>
    <description>Recent content in erklaerbaer on Die wunderbare Welt von Isotopp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Dec 2020 00:00:00 +0000</lastBuildDate><atom:link href="/tags/erklaerbaer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>fork, exec, wait and exit</title>
      <link>/2020/12/28/fork-exec-wait-and-exit.html</link>
      <pubDate>Mon, 28 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/12/28/fork-exec-wait-and-exit.html</guid>
      <description>This is the english version of a 2007 article .
In de.comp.os.unix.linux.misc somebody asked:
  Are commands in a script executed strictly sequentially, that is, will the next command only be executed when the previous command has completed, or will the shell automatically start the next command if the system has spare capacity? Can I change the default behavior - whatever it may be - in any way?   If you are looking into the fine manual, it may explain at some point that the shell starts each command in a separate process.</description>
    </item>
    
    <item>
      <title>An unexpected pool size increase</title>
      <link>/2020/10/07/an-unexpeced-pool-size-increase.html</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/07/an-unexpeced-pool-size-increase.html</guid>
      <description>At work, replication chains have a single primary database node, to which you write, and then multiple replicas, in multiple AZs.
Here is what the one sample chain looks like in Orchestrator:
instance-918d is the current primary, in the blue AZ. Replicas in orange and green are in other AZs. Blue badges indicate multiple replicas, eg (38) means 38 machines.
When you talk to a database, you get two database handles:</description>
    </item>
    
    <item>
      <title>What are the problems with POSIX?</title>
      <link>/2020/10/05/what-are-the-problems-with-posix.html</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/05/what-are-the-problems-with-posix.html</guid>
      <description>Every once in a while there is the IT news article that kind of triggers me. This time it was &amp;ldquo;Object-Storage-Protokoll könnte Posix ablösen&amp;rdquo; in german computer news site Golem . The article speaks about mmap(), NVMEoF and object storage and how it could revolutionize or complete object storages, but does not link to an original article, names no persons and no paper. Also, what do these things - mmap, NVMEoF, object storage and Posix, even have in common?</description>
    </item>
    
    <item>
      <title>MySQL: Import CSV, not using LOAD DATA</title>
      <link>/2020/09/28/mysql-import-csv-not-using-load-data.html</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/28/mysql-import-csv-not-using-load-data.html</guid>
      <description>All over the Internet people are having trouble getting LOAD DATA and LOAD DATA LOCAL to work. Frankly, do not use them, and especially not the LOCAL variant. They are insecure, and even if you get them to work, they are limited and unlikely to do what you want. Write a small data load program as shown below.
Not using LOAD DATA LOCAL   The fine manual says :</description>
    </item>
    
    <item>
      <title>Importing account statements and building a data warehouse</title>
      <link>/2020/09/26/my-private-data-warehouse.html</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/26/my-private-data-warehouse.html</guid>
      <description>This is an update and translation of a much older article , which I wrote in German Language back then. I was experimenting with importing the account statements from my German Sparkasse, which at that time were being made available as a CSV.
The initial data load   The data looked like this:
$ head -2 /home/kris/Documents/banking/umsatz-22758031-29122004.csv &amp;#34;Local Account&amp;#34;;&amp;#34;Book Date&amp;#34;;&amp;#34;Valuta Date&amp;#34;;&amp;#34;Transaction Type&amp;#34;; &amp;#34;Purpose&amp;#34;; &amp;#34;Remote Party&amp;#34;;&amp;#34;Remote Account&amp;#34;;&amp;#34;Bank Code&amp;#34;; &amp;#34;Amount&amp;#34;;&amp;#34;Currency&amp;#34;;&amp;#34;Info&amp;#34; &amp;#34;08154711&amp;#34;;&amp;#34;30.12&amp;#34;;&amp;#34;30.12.05&amp;#34;;&amp;#34;Direct Debit&amp;#34;; &amp;#34;DRP 08154711 040441777 INKL.</description>
    </item>
    
    <item>
      <title>MySQL: automatic partitions surely would be nice</title>
      <link>/2020/09/25/mysql-dynamic-partitions-suck.html</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/25/mysql-dynamic-partitions-suck.html</guid>
      <description>In Deleting data we have been looking at a process that loads data into MySQL, leveraging partitions to make it easier and faster to later get rid of the data again. For this, we created three processes, a data loader process, and two observers - one for creating partitions, and one for deleting them.
The observer processes have been running ANALYZE TABLES and then polling INFORMATION_SCHEMA.PARTITIONS every 1/10th of a second to check if intervention is needed.</description>
    </item>
    
    <item>
      <title>MySQL: Deleting data</title>
      <link>/2020/09/24/mysql-deleting-data.html</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/24/mysql-deleting-data.html</guid>
      <description>Completing the data lifecycle is often harder than originally expected: Deleting data can cost sometimes way more than inserting it in the first place. MySQL Partitions can offer a way out. We have an earlier post on the subject.
A sample table, and a problem statement   Let&amp;rsquo;s define a kind of log table, to which data is added with an auto_increment id value and some data.
#! /usr/bin/env python3 from time import sleep from random import randint from multiprocessing import Process import click import MySQLdb import MySQLdb.</description>
    </item>
    
    <item>
      <title>MySQL: ALTER TABLE for UUID</title>
      <link>/2020/09/22/alter-table-for-uuid.html</link>
      <pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/22/alter-table-for-uuid.html</guid>
      <description>A question to the internal #DBA channel at work: »Is it possible to change a column type from BIGINT to VARCHAR ? Will the numbers be converted into a string version of the number or will be it a byte-wise transition that will screw the values?«
Further asking yielded more information: »The use-case is to have strings, to have UUIDs.«
So we have two questions to answer:
 Is ALTER TABLE t CHANGE COLUMN c lossy?</description>
    </item>
    
    <item>
      <title>MySQL: Encoding fields for great profit.</title>
      <link>/2020/09/18/mysql-encoding-fields-for-great-profit.html</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/18/mysql-encoding-fields-for-great-profit.html</guid>
      <description>Iterating schemas over time is not an uncommon thing. Often requirements emerge only after you have data, and then directed action is possible. Consequently, working on existing data, and structuring and cleaning it up is a common task.
In todays example we work with a log table that logged state transitions of things in freeform VARCHAR fields. After some time the log table grew quite sizeable, and the log strings are repeated rather often, contributing to the overall size of the table considerably.</description>
    </item>
    
    <item>
      <title>MySQL from a Developers Perspective</title>
      <link>/2020/09/07/mysql-from-a-developers-perspective.html</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/07/mysql-from-a-developers-perspective.html</guid>
      <description>So this has turned into a small series, explaining how to work with MYSQL from a developers perspective. This post is intended as a directory for the individual articles. It will be amended and re-dated as necessary.
The code for the series is also available in isotopp/mysql-dev-examples on GitHub.
The Tag #mysqldev will reference all articles from this series.
  MySQL Transactions - the physical side . Looking at how MySQL InnoDB handles transactions on the physical media, enabling rollback and commit.</description>
    </item>
    
    <item>
      <title>MySQL: Generated Columns and virtual indexes</title>
      <link>/2020/09/07/mysql-generated-columns-and-virtual-indexes.html</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/07/mysql-generated-columns-and-virtual-indexes.html</guid>
      <description>We have had a look at how MySQL 8 handles JSON recently, but with all those JSON functions and expressions it is clear that many JSON accesses cannot be fast. To grab data from a JSON column, you will use a lot of $-&amp;gt;&amp;gt;field expressions and similar, and without indexes nothing of this will be fast.
JSON cannot be indexed.
But MySQL 8 offers another feature that comes in handy: Generated columns and indexes on those.</description>
    </item>
    
    <item>
      <title>MySQL: Basic usage of the JSON data type</title>
      <link>/2020/09/04/mysql-json-data-type.html</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/04/mysql-json-data-type.html</guid>
      <description>MySQL 8 provides solid support for the JSON data type. The manual has an overview of the data type , a JSON function reference , an an overview on generated column indexes , and explains multi-values indexes .
Creating JSON columns   Creating JSON columns is easy: Make the column of the JSON data type, fill in valid JSON data.
mysql&amp;gt;createtablet(idintegernotnullprimarykeyauto_increment,jjson);QueryOK,0rowsaffected(0.11sec)mysql&amp;gt;insertintot(j)values-&amp;gt;(&amp;#39;null&amp;#39;),-&amp;gt;(&amp;#39;true&amp;#39;),-&amp;gt;(&amp;#39;false&amp;#39;),-&amp;gt;(&amp;#39;1&amp;#39;),-&amp;gt;(&amp;#39;&amp;#34;keks&amp;#34;&amp;#39;),-&amp;gt;(&amp;#39;[&amp;#34;eins&amp;#34;, &amp;#34;zwei&amp;#34;]&amp;#39;),-&amp;gt;(&amp;#39;{&amp;#34;eins&amp;#34;: &amp;#34;one&amp;#34;, &amp;#34;zwei&amp;#34;: &amp;#34;two&amp;#34;}&amp;#39;);QueryOK,5rowsaffected(0.02sec)mysql&amp;gt;selectjson_type(j)astype,json_valid(j)asvalid,isnull(j)assqlnull,j,idfromt;+---------+-------+---------+--------------------------------+----+ |type|valid|sqlnull|j|id|+---------+-------+---------+--------------------------------+----+ |NULL|NULL|1|NULL|1||NULL|1|0|null|2||BOOLEAN|1|0|true|3||BOOLEAN|1|0|false|4||INTEGER|1|0|1|5||STRING|1|0|&amp;#34;keks&amp;#34;|6||ARRAY|1|0|[&amp;#34;eins&amp;#34;,&amp;#34;zwei&amp;#34;]|7||OBJECT|1|0|{&amp;#34;eins&amp;#34;:&amp;#34;one&amp;#34;,&amp;#34;zwei&amp;#34;:&amp;#34;two&amp;#34;}|8|+---------+-------+---------+--------------------------------+----+ 8rowsinset(0.00sec)mysql&amp;gt;insertintot(j)values(&amp;#39;[&amp;#34;incomplete&amp;#34;, &amp;#34;array&amp;#34;, &amp;#34;closing bracket&amp;#34;&amp;#39;);ERROR3140(22032):InvalidJSONtext:&amp;#34;Missing a comma or &amp;#39;]&amp;#39; after an array element.</description>
    </item>
    
    <item>
      <title>On Touching Candles, And Error Budgets</title>
      <link>/2020/08/31/on-touching-candles.html</link>
      <pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/31/on-touching-candles.html</guid>
      <description>Ok, it&amp;rsquo;s &amp;ldquo;Dad Stories&amp;rdquo; Time (from Twitter ). When my son was somewhat older than a year, he was learning to speak. He could already say &amp;ldquo;Mama&amp;rdquo; and &amp;ldquo;Papa&amp;rdquo;.
It was around Christmas, and there was a candle on the table, glowing interestingly, so he wanted to touch it. &amp;ldquo;Nein, heiß&amp;rdquo; is what you would say in German.
Of course, a toddler does not understand the meaning of &amp;ldquo;heiß&amp;rdquo;. I mean, he&amp;rsquo;s trying to imitate the sound of it, but the meaning of &amp;ldquo;heiß&amp;rdquo; is something specific.</description>
    </item>
    
    <item>
      <title>MySQL: NULL is NULL</title>
      <link>/2020/08/25/null-is-null.html</link>
      <pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/25/null-is-null.html</guid>
      <description>Question: Hey, I got a UNIQUE INDEX, but I can store multiple rows with the same value, NULL. That is surprising. Is that a bug?
 This is a rewrite of the same in German from 9 years ago .
 root@localhost[kris]&amp;gt;createtablet(ainteger,binteger,unique(a,b));QueryOK,0rowsaffected(0.09sec)root@localhost[kris]&amp;gt;insertintotvalues(1,2);QueryOK,1rowaffected(0.01sec)root@localhost[kris]&amp;gt;insertintotvalues(1,2);ERROR1062(23000):Duplicateentry&amp;#39;1-2&amp;#39;forkey&amp;#39;t.a&amp;#39;This does not work, as expected. But this does:
root@localhost[kris]&amp;gt;truncatetablet;QueryOK,0rowsaffected(0.16sec)root@localhost[kris]&amp;gt;insertintotvalues(1,NULL);QueryOK,1rowaffected(0.02sec)root@localhost[kris]&amp;gt;insertintotvalues(1,NULL);QueryOK,1rowaffected(0.03sec)root@localhost[kris]&amp;gt;select*fromt;+------+------+ |a|b|+------+------+ |1|NULL||1|NULL|+------+------+ 2rowsinset(0.00sec)Why is that?
This is usually where I point people at SQL for Smarties: Advanced SQL Programming .</description>
    </item>
    
    <item>
      <title>MySQL: Some Character Set Basics</title>
      <link>/2020/08/18/mysql-character-sets.html</link>
      <pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/18/mysql-character-sets.html</guid>
      <description>This is the updated and english version of some older posts of mine in German. It is likely still incomplete, and will need information added to match current MySQL, but hopefully it is already useful.
Old source articles in German: 1 , 2 and 3 .
Some vocabulary   Symbol, Font, Encoding and Collation - what do they even mean?
A character set is a collection of symbols that belong together.</description>
    </item>
    
    <item>
      <title>ELI5: Epic vs Apple and Google</title>
      <link>/2020/08/14/eli5-epic-vs-apple-and-google.html</link>
      <pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/14/eli5-epic-vs-apple-and-google.html</guid>
      <description>@jpmens asked on Twitter:
 What is Fortnite? #outingmyself
Assuming XKCD 1053 .
Fortnite   Fortnite is a game made by Epic Games. They also make the Unreal Engine, which powers many modern computer games. Fortnite together with Roblox and Minecraft it is the set of games played most by people under 20.
You want to follow Matthew Ball .
Fortnite is a Battle Royal game, a 1st person shooter, in which a matchmaker puts you together with a set of other people of similar skill, drops you into an arena of ever shrinking size.</description>
    </item>
    
    <item>
      <title>MySQL Foreign Key Constraints and Locking</title>
      <link>/2020/08/04/mysql-foreign-key-constraints-and-locking.html</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/04/mysql-foreign-key-constraints-and-locking.html</guid>
      <description>Since we now know how to look at the state of locking in a live database, let&amp;rsquo;s look at what happens when we run a normal insert or update and an insert or update with foreign key relationships defined, and compare.
We will be using the tables and structures from our previous examples, a simple 1:n relationship between a and b:
CREATETABLEa(a_idintNOTNULLAUTO_INCREMENT,PRIMARYKEY(a_id));INSERTINTOaVALUES(10),(20),(30),(40);CREATETABLEb(b_idintNOTNULLAUTO_INCREMENT,a_idintNOTNULL,PRIMARYKEY(b_id),KEY`a_id`(a_id),CONSTRAINTa_id_existsFOREIGNKEY(a_id)REFERENCESa(a_id)ONDELETERESTRICTONUPDATERESTRICT);INSERTINTObVALUES(10,10),(40,40);or the same definition for b without the constraint.</description>
    </item>
    
    <item>
      <title>MySQL Foreign Keys and Foreign Key Constraints</title>
      <link>/2020/08/03/mysql-foreign-keys-and-foreign-key-constraints.html</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/03/mysql-foreign-keys-and-foreign-key-constraints.html</guid>
      <description>Foreign Keys are what links tables together and turns a set of tables into a model. Foreign Key Constraints are conditions that must be true for the content of the tables to be an internally consistent model. Foreign Key Constraints can be defined and enforced in InnoDB, but this comes at a considerable price, and for some it may hurt more than it is worth.
A very simple shop as a ER-model.</description>
    </item>
    
    <item>
      <title>MySQL Deadlocks with INSERT</title>
      <link>/2020/08/02/mysql-deadlocks-with-insert.html</link>
      <pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/02/mysql-deadlocks-with-insert.html</guid>
      <description>Support Channel. &amp;ldquo;Hi, I am getting deadlocks in the database and they occur when I have to rollback the transactions but if we don&amp;rsquo;t have to roll back all transactions get executed.&amp;rdquo; Wait, what? After some back and forth it becomes clear that the Dev experiences deadlocks and has data:
mysql&amp;gt;pagerlessmysql&amp;gt;showengineinnodbstatus\G...MySQLthreadid142531,OSthreadhandle139990258222848,queryid4799571somehost.somedomainsomeuserupdateINSERTintosometable(identifier_id,currency,balance)VALUES(&amp;#39;d4e84cb1-4d56-4d67-9d16-1d548fd26b55&amp;#39;,&amp;#39;EUR&amp;#39;,&amp;#39;0&amp;#39;)***(2)HOLDSTHELOCK(S):RECORDLOCKSspaceid3523pageno1106463nbits224indexPRIMARYoftable`somedb`.`sometable`trxid9843342279lockmodeSlocksgapbeforerecand that is weird because of the lock mode S locks gap in the last line. We get the exact same statement with the exact same value on the second thread, but with lock mode X locks gap.</description>
    </item>
    
    <item>
      <title>MySQL: Locks and Deadlocks</title>
      <link>/2020/08/01/mysql-locks-and-deadlocks.html</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/01/mysql-locks-and-deadlocks.html</guid>
      <description>In a previous article we wrote data to the database using atomic update statements, and then using transactions with SELECT ... FOR UPDATE. In this article we will look at what happens when we continue doing this, in a more complicated way. Source code for this article is also available on github.com .
A simple row lock   But first let&amp;rsquo;s do things manually: We create a table kris with an integer primary key column and a secondary unindexed data column.</description>
    </item>
    
    <item>
      <title>MySQL Transactions - writing data</title>
      <link>/2020/07/30/mysql-transactions-writing-data.html</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/30/mysql-transactions-writing-data.html</guid>
      <description>Using the framework for testing we created in earlier articles, let&amp;rsquo;s try to modify some data. We are writing a small program that increments a counter. Our table looks like this, and contains 10 counters:
CREATETABLE`demo`(`id`bigintunsignedNOTNULLAUTO_INCREMENT,`counter`intNOTNULLDEFAULT&amp;#39;0&amp;#39;,UNIQUEKEY`id`(`id`))INSERTINTO`demo`VALUES(1,0);INSERTINTO`demo`VALUES(2,0);...INSERTINTO`demo`VALUES(10,0);We are using some very simple programming to increment a counter:
@sql.command() @click.option(&amp;#34;--name&amp;#34;, default=&amp;#34;demo&amp;#34;, help=&amp;#34;Table name to count in&amp;#34;) @click.option(&amp;#34;--id&amp;#34;, default=0, help=&amp;#34;Counter to use&amp;#34;) @click.option(&amp;#34;--count&amp;#34;, default=1000, help=&amp;#34;Number of increments&amp;#34;) def count(name, id, count): &amp;#34;&amp;#34;&amp;#34; Increment counter --id by --count many steps in table --name &amp;#34;&amp;#34;&amp;#34; for i in range(0, count): cmd = f&amp;#34;update {name}set counter=counter+1 where id = {id}&amp;#34; c = db.</description>
    </item>
    
    <item>
      <title>MySQL Commit Size and Speed</title>
      <link>/2020/07/27/mysql-commit-size-and-speed.html</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/27/mysql-commit-size-and-speed.html</guid>
      <description>When writing data to disk, for small transactions the cost of writing the commit out do disk dominates the execution time of the script. In order to show that, I wrote a little bit of Python.
The script creates a test table in a database and writes 10.000 rows of test data into it, in commit sizes of 1, 2, 4, &amp;hellip;, 1024 rows.
$ ./mysql.py --help Usage: mysql.py [OPTIONS] COMMAND [ARGS].</description>
    </item>
    
    <item>
      <title>MySQL Transactions - the physical side</title>
      <link>/2020/07/27/mysql-transactions.html</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/27/mysql-transactions.html</guid>
      <description>So you talk to a database, doing transactions. What happens actually, behind the scenes? Let’s have a look.
There is a test table and we write data into it inside a transaction:
CREATETABLEt(idserial,datavarbinary(255))STARTTRANSACTIONREADWRITEINSERTINTOt(id,data)VALUES(NULL,RANDOM_BYTES(255))COMMITThe MySQL test instance we are talking to is running on a Linux machine, and otherwise idle to make observation easier. Also, we configured it with innodb_use_native_aio = false because observing actual physical asynchronous I/O and attributing it to the statement that caused it is really hard.</description>
    </item>
    
    <item>
      <title>MySQL Window Functions</title>
      <link>/2020/06/21/mysql-window-functions.html</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/21/mysql-window-functions.html</guid>
      <description>Two questions from Reddit&amp;rsquo;s /r/mysql related to Window Functions: How do I make row.numbers happen and Get the difference between two values in different recordings .
One of the new things in MySQL is the implementation of Window Functions. They are related to aggregates, but do not actually lump values together.
To better understand what goes on, let&amp;rsquo;s create some fake data to work with:
#! /usr/bin/env python3 # -*- coding: utf-8 -*- import MySQLdb as mdb import MySQLdb.</description>
    </item>
    
    <item>
      <title>Export the entire database to CSV</title>
      <link>/2020/06/20/export-the-entire-database-to-csv.html</link>
      <pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/20/export-the-entire-database-to-csv.html</guid>
      <description>A question from Reddit&amp;rsquo;s /r/mysql:
 Really new to MySQL and had a request to export an entire database to csv for review. I can manually export each table using workbench but there are 10+ tables and 10+ databases so I was looking to export the entire database to csv.
 It is likely that you have additional requirements on top of this, so it would be best to script this in a way that would allow for customization.</description>
    </item>
    
    <item>
      <title>Pizza, People, Projects and Processes</title>
      <link>/2020/06/15/pizza-people-projects-and-processes.html</link>
      <pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/15/pizza-people-projects-and-processes.html</guid>
      <description>An older talk from 2 years ago, which for some reason I was not able to find in the blog.
For reasons that do not need exploration at this junction I had to explain Processes and Process Maturity some time ago, and a colleague asked me to put my thinking into a talk. So this is going to be boring, because you are likely to know most of this already, but on the other hand, it is good to be on the same page when it comes to models and vocabulary.</description>
    </item>
    
    <item>
      <title>Cloud and Energy</title>
      <link>/2020/06/08/cloud-and-energy.html</link>
      <pubDate>Mon, 08 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/08/cloud-and-energy.html</guid>
      <description>In Data Centers and Energy I wrote about Hyperscaler Data Centers and Open Compute, and how they bring down the PUE of data centers, making them more efficient, and in Streaming and Energy I followed up on this, explaining how Netflix energy usage fits into this. Now the Uptime Institute has released a study that claims &amp;ldquo;Data center energy efficiency gains have flattened out&amp;rdquo;.
It is summarized as:
 The average power usage effectiveness (PUE) ratio for a data center in 2020 is 1.</description>
    </item>
    
    <item>
      <title>Optimistic locking</title>
      <link>/2020/06/04/optimistic-locking.html</link>
      <pubDate>Thu, 04 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/04/optimistic-locking.html</guid>
      <description>A question from Reddit&amp;rsquo;s /r/mysql:
 Hey, I was planning to make a dashboard, where Users are subjected to make edits on their profiles every now and then, and I expect a high volume of requests to the database.
Having worked previously with MySQL for another Dashboard, I encountered errors for:
  Maximum user connections - when I connected to the database only while query was to be executed</description>
    </item>
    
    <item>
      <title>MySQL store_result and use_result</title>
      <link>/2020/06/01/mysql-storeresult-and-useresult.html</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/01/mysql-storeresult-and-useresult.html</guid>
      <description>A question from Reddit’s /r/mysql led to a longer discussion of cursors and how they are implemented in MySQL, and what the advantages and drawbacks are.
The OP probably had a slow query, but was misphrasing the question:
 I am having performance problems where my cursor is taking 6-8 seconds to go through a hundred rows. I think this is because I am also using while loops to loop through JSON objects to pull information.</description>
    </item>
    
    <item>
      <title>Streaming and Energy</title>
      <link>/2019/12/28/streaming-and-energy.html</link>
      <pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/28/streaming-and-energy.html</guid>
      <description>A bunch of boomers in Germany is running a distraction campaign on the energy use of data centers and streaming. Example articles in german language can be found in Zeit and Bento , but there is a larger series of articles acrooss multiple newspapers.
A better structured reasoning can be found in SRF (German), and it highlights how arbitrary and wrong the energy numbers in the former articles are. But even this article ignores the facts that the energy consumption in a typical cloud data center is most likely carbon neutral, because the power used is likely to be completely green.</description>
    </item>
    
    <item>
      <title>Trying lvmraid for real</title>
      <link>/2019/12/03/trying-lvmraid-for-real.html</link>
      <pubDate>Tue, 03 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/03/trying-lvmraid-for-real.html</guid>
      <description>So after testing LVM Raid in princple, I have been trying it on some real hardware to see what happens. The idea was to estimate if it scales and if not, how it doesn&amp;rsquo;t. I was expecting to run into all kinds of obscure problems in my testing, but in fact, it was a quick and short death.
Here is my box: QuantaGrid D42A-2U with an AMD EPYC 7551P CPU (32C, HT off), 1024 GB of memory, a boot disk and 12x Micron_9200_MTFDHAL11TATCW (PDF ) for 120TB of disk storage.</description>
    </item>
    
    <item>
      <title>Cloning and splitting logical volumes</title>
      <link>/2019/12/02/cloning-and-splitting-logical-volumes.html</link>
      <pubDate>Mon, 02 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/02/cloning-and-splitting-logical-volumes.html</guid>
      <description>Where I work, we routinely run our databases on XFS on LVM2.
The setup   Each database has their database on /mysql/schemaname, with the subdirectories /mysql/schemaname/{data,log,tmp}. The entire /mysql/schemname tree is a LVM2 Logical Volume mysqlVol on the Volume Group vg00, which is then formatted as an XFS filesystem.
# pvcreate /dev/nvme*n1 # vgcreate vg00 /dev/nvme*n1 # lvcreate -n mysqlVol -L...G vg00 # mkfs -t xfs /dev/vg00/mysqlVol # mount -t xfs /dev/vg00/mysqlVol /mysql/schemaname Basic Ops   You can grow an existing LVM Logical Volume with lvextend -L+50G /dev/vg00/mysqlVol or similar, and then xfs_grow /dev/vg00/myqlVol.</description>
    </item>
    
    <item>
      <title>Data Centers and Energy</title>
      <link>/2019/10/05/data-centers-and-energy.html</link>
      <pubDate>Sat, 05 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/10/05/data-centers-and-energy.html</guid>
      <description>Deutsche Welle is shocked: Generation Greta is watching Netflix (Article in German Language), Netflix runs on computers, and apparently computers are using power.
Let&amp;rsquo;s have a look.
EDIT: Second part Streaming and Energy now available.
End-User Device.   It&amp;rsquo;s 2019. End user devices are using Wi-Fi, and are running on Batteries. They are Cellphones, Tablets or Laptops.
Devices that are not connected to grid are more usable if they are trying to save energy, and so all modern devices are full of special purpose hardware that allows them to fulfill their main functions more energy efficient.</description>
    </item>
    
    <item>
      <title>Konsenssysteme</title>
      <link>/2019/09/03/konsenssysteme.html</link>
      <pubDate>Tue, 03 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/03/konsenssysteme.html</guid>
      <description>Ein Thread über Konsenssysteme aus Twitter Mehr als ein Key-Value Store   Heise schreibt: Verteilter Key-Value-Store: etcd erreicht Version 3.4 . etcd ist auch ein Key-Value-Store, aber das ist nur ein Nebendetail. Die Beschreibung der neuen Funktionen im Artikel macht auch schon keinen Sinn für einen KV-Store.
etcd ist ein Konsenssystem. Es realisiert Clustermitgliedschaft, verteilte Locks, und darauf aufbauende Dienste wie Service Discovery und Loadbalancing.
Es gibt drei von Kyle Kingsbury getestete Implementierungen von Konsenssystemen, die funktionieren: Zookeeper, etcd und consul.</description>
    </item>
    
    <item>
      <title>But is it atomic?</title>
      <link>/2018/11/29/but-is-it-atomic.html</link>
      <pubDate>Thu, 29 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/11/29/but-is-it-atomic.html</guid>
      <description>From Pluspora But is it atomic?   So a few days ago, a colleague asked “Why do we love files on disk?” and in the course of that discussion, I made a comment that, among other things, used the assumption that somebody is updating some file on some Linux system atomically. I wrote:
Let&amp;rsquo;s assume we are using local files, and we do so in a managed, sane way:</description>
    </item>
    
    <item>
      <title>beep, patch and ed</title>
      <link>/2018/04/06/beep-patch-and-ed.html</link>
      <pubDate>Fri, 06 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/06/beep-patch-and-ed.html</guid>
      <description>So a few days ago, somebody found an exploit in beep - now CVE-2018-0492 .
beep is a program that is part of Debian (and Ubuntu) to have the PC speaker multiple times, at different frequencies, with different pauses and beep lengths. That works just fine. It&amp;rsquo;s also SUID root. There is zero code in it that deals with the fact that it may run privileged.
The author confidently writes:</description>
    </item>
    
    <item>
      <title>Hashes in Structures</title>
      <link>/2018/03/04/hashes-in-structures.html</link>
      <pubDate>Sun, 04 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/04/hashes-in-structures.html</guid>
      <description>In Hashes and their uses we have been talking about hash functions in general, and cryptographic hashes in particular. We wanted four things from cryptographic hashes:
 The hash should be fast to calculate on a large string of bytes. The hash is slow to reverse (i.e. only by trying all messages and checking each result). The hash is slow to find collisions for (i.e. it&amp;rsquo;s hard to find two input strings that have the same hash value).</description>
    </item>
    
    <item>
      <title>Hashes and their uses</title>
      <link>/2018/02/26/hashes-and-their-uses.html</link>
      <pubDate>Mon, 26 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/26/hashes-and-their-uses.html</guid>
      <description>A hash function is a function that maps a large number of arbitrary data types onto a smaller number of contiguous integers.
This simple hash function maps strings of arbitrary length to integers. Some strings are mapped to the same integer: a hash value collision.
The base set here is a number of strings of arbitrary length, which is a theoretically open ended set size. The target is a bounded number of integer values.</description>
    </item>
    
    <item>
      <title>A Journey to Open Compute</title>
      <link>/2018/02/21/a-journey-to-open-compute.html</link>
      <pubDate>Wed, 21 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/21/a-journey-to-open-compute.html</guid>
      <description>Yesterday, Booking.com hosted the Open Compute Meetup in Amsterdam. My talk is on Slideshare and a recording is on Youtube.
 Slideshare: A journey to OCP  Youtube: A journey to OCP   A cleaned up and more coherent transcript of the talk is here: Booking.com started out as a small online travel agency in Amsterdam, but is now financially about 15% of Google. We have about 200 offices in 70 countries, support 46 languages and reach about any touristically interesting spot on this planet.</description>
    </item>
    
    <item>
      <title>Conway&#39;s Law</title>
      <link>/2018/01/12/conways-law.html</link>
      <pubDate>Fri, 12 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/12/conways-law.html</guid>
      <description>Melvin Conway is a compiler developer and systems designer, who is well known for the eponymous Conway&amp;rsquo;s Law . Various phrasings exist of that, and one popular is
 Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations.
 The original paper and an introductory paragraph can be found on his website . It&amp;rsquo;s worth reading, because there are more useful insights to be found in the original writeup.</description>
    </item>
    
    <item>
      <title>Community Management?</title>
      <link>/2017/08/23/community-management.html</link>
      <pubDate>Wed, 23 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/23/community-management.html</guid>
      <description>Today is a weird day. First thing is a friend asking about help with community management. And next thing is Fefe reiterating his longstanding fallacy (Rant in German ) that programmers are able to do anything just because they are able to do one thing (here: Community Management). The TL;DR is that he rants against non-programmers showing interest into programming projects because they like the software, thereby ruining everything by not being programmers.</description>
    </item>
    
    <item>
      <title>PHP: Understanding unserialize()</title>
      <link>/2017/08/11/php-understanding-unserialize.html</link>
      <pubDate>Fri, 11 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/11/php-understanding-unserialize.html</guid>
      <description>The history of serialize() and unserialize() in PHP begins with Boris Erdmann and me, and we have to go 20 years back in time. This is the day of the prerelease versions of PHP 3, some time in 1998 .
Boris and I were working on code for a management system for employee education for German Telekom. The front side is a web shop that sells classes and courses, the back end is a complex structure that manages attendance, keeps track of a line manager approval hierarchy and provides alternative dates for overfull classes.</description>
    </item>
    
    <item>
      <title>Monitoring - the data you have and the data you want</title>
      <link>/2017/08/09/monitoring-the-data-you-have-and-the-data-you-want.html</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/09/monitoring-the-data-you-have-and-the-data-you-want.html</guid>
      <description>So you are running systems in production and you want to collect data from your systems. You need to build a monitoring system. That won&amp;rsquo;t work and it won&amp;rsquo;t scale. So please stop for a moment, and think. What kind of monitoring do you want do build? I know at least three different types of monitoring system, and they have very different objectives, and consequently designs.
Three types of Monitoring Systems   The first and most important system you want to have is checking for incidents.</description>
    </item>
    
    <item>
      <title>Scaling, automatically and manually</title>
      <link>/2017/08/09/scaling-automatically-and-manually.html</link>
      <pubDate>Wed, 09 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/09/scaling-automatically-and-manually.html</guid>
      <description>There is an interesting article by Brendan Gregg out there, about the actual data that goes into the Load Average metrics of Linux. The article has a few funnily contrasting lines. Brendan Gregg states
 Load averages are an industry-critical metric – my company spends millions auto-scaling cloud instances based on them and other metrics […]
 but in the article we find Matthias Urlichs saying
 The point of &amp;ldquo;load average&amp;rdquo; is to arrive at a number relating how busy the system is from a human point of view.</description>
    </item>
    
    <item>
      <title>So you want to write a Shell script</title>
      <link>/2017/08/07/so-you-want-to-write-a-shell-script.html</link>
      <pubDate>Mon, 07 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/08/07/so-you-want-to-write-a-shell-script.html</guid>
      <description>So some people, companies even, have guidelines that describe how to write shell scripts , or even unit tests for shell scripts , as if &amp;ldquo;UNIX Shell&amp;rdquo; was a programming language.
That&amp;rsquo;s wrong.
&amp;ldquo;Modern Shells&amp;rdquo; are based on a language that has been written without a formal language specification. The source looked like this , because somebody didn&amp;rsquo;t like C and wanted Algol, abusing the preprocessor . The original functionality and language rules had to be reverse engineered from that source, and original shell has a lot of weird rules and quirks :</description>
    </item>
    
    <item>
      <title>Zero Factor Authentication</title>
      <link>/2017/07/27/zero-factor-authentication.html</link>
      <pubDate>Thu, 27 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/27/zero-factor-authentication.html</guid>
      <description>Dear Internet, Today I Learned that oath-toolkit exists in Homebrew. So, this is a thing:
$ brew install oath-toolkit $ alias totp=&amp;#39;oathtool --totp -b YOURSECRET32BLA | pbcopy&amp;#39; And so is this:
#! /usr/bin/env expect -f # exp_internal 1 set seed [ exec security find-generic-password -w -a $USER -s seed ] set totp [ exec oathtool --totp -b $seed ] set pass [ exec security find-generic-password -w -a $USER -s pass ] spawn ssh verysecure.</description>
    </item>
    
    <item>
      <title>Using MySQL Partitions (a Python example)</title>
      <link>/2017/07/09/using-mysql-partitions-a-python-example.html</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/09/using-mysql-partitions-a-python-example.html</guid>
      <description>Today somebody had a problem with expiring a large table (a Serendipity Blog table).
In MySQL InnoDB, tables are physically ordered by primary key (InnoDB data is a B+ tree, a balanced tree where the data pages are the leaves of the tree). If you are expiring old data from such a log table, you are deleting from the left hand side of the tree, and since it is a balanced tree, that triggers a lot of rebalancing - hence it is very slow.</description>
    </item>
    
    <item>
      <title>New Technology vs Planned Obsolescence</title>
      <link>/2017/07/07/new-technology-vs-planned-obsolescence.html</link>
      <pubDate>Fri, 07 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/07/new-technology-vs-planned-obsolescence.html</guid>
      <description>based on an old Google plus article from 2015: What you observe as Planned Obsolescence is often the natural outcome of fast product cycles that are necessary for any new technology. When a new thing arrives in the market, it is often barely viable, a minimum viable product .
We are remembering the iPhone 1 as revolutionary, but we chose to forget about is slowness, its clunkyness and the very limited feature set it had.</description>
    </item>
    
    <item>
      <title>Rolling out patches and changes, often and fast</title>
      <link>/2017/05/18/rolling-out-patches-and-changes-often-and-fast.html</link>
      <pubDate>Thu, 18 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/18/rolling-out-patches-and-changes-often-and-fast.html</guid>
      <description>Fefe had a short pointer to an article Patching is Hard . It is, but you can make it a lot easier by doing a few things right. I did s small writeup (in German ) to explain this, which Fefe posted.
I do have an older talk on this, titled &amp;ldquo;8 rollouts a day&amp;rdquo; (more like 30 these days). There are slides and a recording . The Devops talk &amp;ldquo;Go away or I will replace you with a little shell script&amp;rdquo; addresses it, too, but from a different angle (slides , recording ).</description>
    </item>
    
    <item>
      <title>Handling Wannacrypt - a few words about technical debt</title>
      <link>/2017/05/15/handling-wannacrypt-a-few-words-about-technical-debt.html</link>
      <pubDate>Mon, 15 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/15/handling-wannacrypt-a-few-words-about-technical-debt.html</guid>
      <description>So Microsoft had a bug in their systems. Many of their sytems. For many years. That happens. People write code. These people write bugs. Microsoft over the years has become decently good with fixing bugs and rolling out upgrades, quickly. That&amp;rsquo;s apparently important, because we all are not good enough at not writing bugs. So if we cannot prevent them, we need to be able to fix them and then bring these fixes to the people.</description>
    </item>
    
    <item>
      <title>node.js idea of an inode is approximately broken</title>
      <link>/2017/04/19/node-js-idea-of-an-inode-is-approximately-broken.html</link>
      <pubDate>Wed, 19 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/04/19/node-js-idea-of-an-inode-is-approximately-broken.html</guid>
      <description>The Tweet points to the bug report . After the facepalming there is still a lot to say about that.
About the bug:   There is a system call stat(2) in Posix, which returns a struct stat as a result. Part of that data structure is a field st_ino, which contains the inode number of that file. That number is a unique file identifier, a 64 bit bit pattern.</description>
    </item>
    
    <item>
      <title>Containers 101</title>
      <link>/2017/02/17/containers-101.html</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/02/17/containers-101.html</guid>
      <description>It is helpful to remember that containers are just normal Unix processes with two special tricks.
Normal Unix Processes   Unix starts processes by performing a fork() system call to create a new child process.
The child process still contains the same program as the parent process, so the parent processes program still has control over the child. It usually performs a number of operations within the context of the new child, preparing the environment for the new program, from within.</description>
    </item>
    
    <item>
      <title>Load, Load Testing and Benchmarking</title>
      <link>/2017/02/16/load-load-testing-and-benchmarking.html</link>
      <pubDate>Thu, 16 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/02/16/load-load-testing-and-benchmarking.html</guid>
      <description>(This article also available in german language .)
So you have a new system and want to know what the load limits are. For that you want to run a benchmark.
Basic Benchmarking   The main plan looks like this:
The basic idea: Find a box, offer load, see what happens, learn.
You grab a box and find a method to generate load. Eventually the box will be fully loaded and you will notice this somehow.</description>
    </item>
    
    <item>
      <title>FOSDEM Talk: cgroups-v2</title>
      <link>/2017/02/07/fosdem-talk-cgroups-v2.html</link>
      <pubDate>Tue, 07 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/02/07/fosdem-talk-cgroups-v2.html</guid>
      <description>The talk The cgroups-v2 API is incompatible with cgroups-v1. This is a good thing, because the initial use cases for cgroups have not been fleshed out properly, and the API is kind of overly complicated from the POV of the consumers. v2 is being built with use cases and experience in mind, and is much easier to use. Control groups allow Linux users to limit the kind and amount of resources being used, which is employed by systemd, container drivers and many other things.</description>
    </item>
    
    <item>
      <title>Command line access to the Mac keychain</title>
      <link>/2017/01/26/command-line-access-to-the-mac-keychain.html</link>
      <pubDate>Thu, 26 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/01/26/command-line-access-to-the-mac-keychain.html</guid>
      <description>I am getting my payslips in electronic form, as an encrypted, password protected PDF. It&amp;rsquo;s not a super secret password, and the encryption is more against accidentally opening the file than it is to keep the content of the file actually secret.
After shipping the PDF home, I am archiving it for tax purposes, but in order to make the archival safe, I am storing the original file as well as the decrypted cleartext version of it.</description>
    </item>
    
    <item>
      <title>Internet mit Kugelschreibern (und ein Päckchen für Mama)</title>
      <link>/2009/06/27/internet-mit-kugelschreibern-und-ein-p-ckchen-f-r-mama.html</link>
      <pubDate>Sat, 27 Jun 2009 00:00:00 +0000</pubDate>
      
      <guid>/2009/06/27/internet-mit-kugelschreibern-und-ein-p-ckchen-f-r-mama.html</guid>
      <description>Ich bin ja ein guter Sohn und schicke daher meiner Mama ab und an ein Päckchen. Da freut sie sich immer. Weil ich außerdem ein Geek bin, ist es ein IP-Päckchen.
Auf so ein Päckchen muß natürlich auch ein Adreßaufkleber drauf, und in dem müssen die wichtigen Zustellinformationen eingetragen werden.
Vordruck für den Versand von IP-Päckchen, korrekt ausgefüllt und mit ein wenig TTL frankiert.
Für die Zustellung sind drei Felder im Adreßformular wichtig: In das Feld &amp;lsquo;32 Bit destination IP address&amp;rsquo; ist die Empfängeranschrift einzutragen, hier also beispielhaft mal 134.</description>
    </item>
    
    <item>
      <title>Connection Scoped State bei MySQL</title>
      <link>/2009/04/19/connection-scoped-state-bei-mysql.html</link>
      <pubDate>Sun, 19 Apr 2009 00:00:00 +0000</pubDate>
      
      <guid>/2009/04/19/connection-scoped-state-bei-mysql.html</guid>
      <description>Aus einer Diskussion in der deutschsprachigen MySQL Gruppe im USENET. Dort ging es um die Frage, warum phpMyAdmin ein eingeschränktes Werkzeug ist und bei vielen Helfern im Netz unbeliebt. Meine Antwort lautete so:
phpMyAdmin unterliegt wie auch viele grafische Werkzeuge für MySQL (darunter auch jene, die von MySQL selbst bereitgestellt werden) einigen besonderen Einschränkungen. Diese sind prinzipbedingt und daher auch nicht leicht zu beheben.
Aber von vorne:
In MySQL ist es so, daß die Connection einen besonderen Kontext oder Scope darstellt.</description>
    </item>
    
    <item>
      <title>fork, exec, wait und exit</title>
      <link>/2007/01/07/fork-exec-wait-und-exit.html</link>
      <pubDate>Sun, 07 Jan 2007 00:00:00 +0000</pubDate>
      
      <guid>/2007/01/07/fork-exec-wait-und-exit.html</guid>
      <description>In de.comp.os.unix.linux.misc fragte jemand:
  Werden in einem Skript die Befehle streng sequentiell ausgeführt, d.h. der nächste erst bearbeitet, wenn der Vorgänger vollständig ausgeführt ist, oder wird automatisch bei unvollständiger Auslastung des Systems bereits der nächste Befehl angefangen? Läßt sich das Standardverhalten - wie auch immer es sein mag - bei Bedarf ändern?   Wenn man in ein Shellbuch schaut, wird einem an der einen oder anderen Stelle möglicherweise erläutert, daß die Shell jeden Befehl in einem eigenen Prozeß abarbeitet.</description>
    </item>
    
    <item>
      <title>Filesysteme für theclaw (30 Jahre Unix Source)</title>
      <link>/2006/12/26/filesysteme-fuer-theclaw-30-jahre-unix-source.html</link>
      <pubDate>Tue, 26 Dec 2006 00:00:00 +0000</pubDate>
      
      <guid>/2006/12/26/filesysteme-fuer-theclaw-30-jahre-unix-source.html</guid>
      <description>&amp;mdash; Log opened Di Dez 26 15:52:09 2006
theclaw&amp;gt; Hey :] Spitze erklaerung zu ext2. Isotopp&amp;gt; Danke
theclaw&amp;gt; Bist du Kerneldeveloper?
Isotopp&amp;gt; Nein. Mysql Consultant.
theclaw&amp;gt; Hmm. Hab da was nicht verstanden bei der Erklärung. Und zwar: Was sind Datenblockzeiger?
Isotopp&amp;gt; Die Blockadressen von Datenblöcken einer Datei.
theclaw&amp;gt; Ich paste mal was
(0-11):9711-9722, (IND):9723, (12-267):9724-9979, (DIND):9980, (IND):9981, (268-523):9982-10237, (IND):10238, (524-779):10239-10494, (IND):10495, (780-1035):10496-10751, (IND):10752, (1036-1291):10753-11008, (IND):11009, (1292-1547):11010-11265, (IND):11266, (1548-1795):11267-11514 Isotopp&amp;gt; Habs im Originalartikel .</description>
    </item>
    
    <item>
      <title>Mein privates Datawarehouse - Sparen mit MySQL</title>
      <link>/2006/07/23/mein-privates-datawarehouse-sparen-mit-mysql.html</link>
      <pubDate>Sun, 23 Jul 2006 00:00:00 +0000</pubDate>
      
      <guid>/2006/07/23/mein-privates-datawarehouse-sparen-mit-mysql.html</guid>
      <description>Meine Sparkasse exportiert mir die Kontoauszüge aus Wunsch auch als CSV. Die Dateien sehen so aus:
&amp;#34;Auftragskonto&amp;#34;;&amp;#34;Buchungstag&amp;#34;;&amp;#34;Valutadatum&amp;#34;;&amp;#34;Buchungstext&amp;#34;;&amp;#34;Verwendungszweck&amp;#34;;&amp;#34;Begünstigter/Zahlungspflichtiger&amp;#34;;&amp;#34;Kontonummer&amp;#34;;&amp;#34;BLZ&amp;#34;;&amp;#34;Betrag&amp;#34;;&amp;#34;Währung&amp;#34;;&amp;#34;Info&amp;#34;&amp;#34;08154711&amp;#34;;&amp;#34;30.12&amp;#34;;&amp;#34;30.12.05&amp;#34;;&amp;#34;LASTSCHRIFT&amp;#34;;&amp;#34;DRP 08154711 040441777 INKL. 16% UST 5.38 EUR&amp;#34;;&amp;#34;STRATO MEDIEN AG&amp;#34;;&amp;#34;040441777&amp;#34;;&amp;#34;10050000&amp;#34;;&amp;#34;-39,00&amp;#34;;&amp;#34;EUR&amp;#34;;&amp;#34;Umsatz gebucht&amp;#34;Weil ich wissen will, wofür ich mein Geld ausgebe, lade ich diese Daten in ein MySQL.
Das geht so:
Zunächst einmal muß ich mir eine Tabelle definieren, in die ich den Load vornehmen kann. Diese Tabelle hat Felder, die in erster Linie dazu geschaffen sind, die Daten aufnehmen zu können.</description>
    </item>
    
    <item>
      <title>Fragmentierung (für Jannik)</title>
      <link>/2006/05/08/fragmentierung-fuer-jannik.html</link>
      <pubDate>Mon, 08 May 2006 00:00:00 +0000</pubDate>
      
      <guid>/2006/05/08/fragmentierung-fuer-jannik.html</guid>
      <description>ircnet, #lug-kiel, am 7. und 8. Mai.
Jannik wundert sich über fsck&amp;rsquo;s Meldung
/: 130834/6553600 Dateien (1.1% nicht zusammenhängend), 1007700/13107200 Blöcke Ist das schlimm? Jannik: hmm.. /:
130834/6553600 Dateien (1.1% nicht zusammenhängend), 1007700/13107200 Blöcke Sieht so aus als wäre mein Dateisystem fragmentiert gewesen o.O? Ich dachte, so etwas kennt Linux nicht.
tholle: Linux oder das Dateisystem? ;-)
Isotopp: Seufz. &amp;ldquo;1.1% nicht zusammenhängend&amp;rdquo; Natürlich können Dateien in nicht fortlaufenden Blocknummern gespeichert sein.</description>
    </item>
    
    <item>
      <title>User und Gruppen, Prozesse und Dateien</title>
      <link>/2005/11/01/user-und-gruppen-prozesse-und-dateien.html</link>
      <pubDate>Tue, 01 Nov 2005 00:00:00 +0000</pubDate>
      
      <guid>/2005/11/01/user-und-gruppen-prozesse-und-dateien.html</guid>
      <description>Heute im Irc stellte eine Teilnehmerin den folgenden Fragenschwall:
 Wie finde ich eigentlich heraus, was für Gruppen es auf einem Linuxsystem gibt? Wie füge ich da jemanden hinzu? Lege ich den zuerst als User an, ganz normal? Und: Wenn ich einen Ordner anlege, der nur für eine bestimmte Gruppe zugänglich sein soll, mache ich das doch über File Permissions, oder habe ich falsch gedacht?
 Die offensichtliche Antwort, das Nachsehen in /etc/group, funktioniert bei modernen Unixen nicht mehr zwingend, denn Gruppendefinitionen können nicht nur in lokalen Dateien stehen, sondern auch aus dem NIS, dem NIS+, einem LDAP oder einem Active Directory kommen.</description>
    </item>
    
    <item>
      <title>Dynamisch geladener Code</title>
      <link>/2005/10/08/dynamisch-geladener-code.html</link>
      <pubDate>Sat, 08 Oct 2005 00:00:00 +0000</pubDate>
      
      <guid>/2005/10/08/dynamisch-geladener-code.html</guid>
      <description>Inzwischen bin ich so weit, daß ich viele Unix-Kommandozeilenprogramme zwar nützlich finde, aber in einem größeren Maßstab als unhandlich und schlecht wiederzuverwenden ansehe. Das liegt daran, daß das Konzept der Pipeline und der Kommandozeile zwar sehr mächtig sind, insbesondere wenn man sie mit einer guten Shell verwendet, aber einen nur so weit bringen.
Manchmal muß man doch richtigen Code schreiben, und wenn man dann den Compiler oder auch nur eine Scriptsprache schwingen muß, dann nützen einem die ganzen Kommandozeilen-Utilities gar nichts mehr.</description>
    </item>
    
    <item>
      <title>Netzmasken über den Augen</title>
      <link>/2004/09/14/netzmasken-ueber-den-augen.html</link>
      <pubDate>Tue, 14 Sep 2004 00:00:00 +0000</pubDate>
      
      <guid>/2004/09/14/netzmasken-ueber-den-augen.html</guid>
      <description>Heute habe ich einem jüngeren Kollegen Netmasks und Broadcast-Adressen beibringen wollen, und habe eine Runde über Stellenwertsysteme, Logarithmen, den Unterschied zwischen Ziffer und Zahl und dergleichen Dinge mehr drehen müssen. Mir sind diese Dinge damals mehr oder weniger zugeflogen - einmal weil so etwas an der Schule unterrichtet worden ist, und zum anderen, weil ich einige Jahre mit verschiedenen Assemblern rumhantiert habe, bevor ich meine erste Hochsprache (C64 Basic zählt wohl nicht als solche :) gelernt habe.</description>
    </item>
    
  </channel>
</rss>
