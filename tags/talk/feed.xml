<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>talk on Die wunderbare Welt von Isotopp</title>
    <link>https://blog.koehntopp.info/tags/talk.html</link>
    <description>Recent content in talk on Die wunderbare Welt von Isotopp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Feb 2018 00:00:00 +0000</lastBuildDate><atom:link href="https://blog.koehntopp.info/tags/talk/feed.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Journey to Open Compute</title>
      <link>https://blog.koehntopp.info/2018/02/21/a-journey-to-open-compute.html</link>
      <pubDate>Wed, 21 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2018/02/21/a-journey-to-open-compute.html</guid>
      <description>&lt;p&gt;Yesterday, Booking.com hosted the Open Compute Meetup in
Amsterdam. My talk is on Slideshare and a recording is on
Youtube.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/Screen-Shot-2018-02-21-at-09.54.18.png&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.slideshare.net/isotopp/a-journey-to-ocp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slideshare: A journey to OCP&lt;/a&gt;

&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=c0Z32UsB5g0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Youtube: A journey to OCP&lt;/a&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A cleaned up and more coherent transcript of the talk is here:
&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.003.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Booking.com started out as a small online travel agency in
Amsterdam, but is now financially about 15% of Google. We have
about 200 offices in 70 countries, support 46 languages and
reach about any touristically interesting spot on this planet.&lt;/p&gt;
&lt;p&gt;We started out selling rooms on a comission basis, but since the
Priceline Group of companies also includes Priceline for
flights, Rentalcars for, well rental cars, and Open Table for
restaurants, it makes sense to integrate that more and graduate
from a Hotel Room marketplace to something more complete, a full
trip or complete experience marketplace.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.004.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;At this point we have about 30k machines in 3 locations.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s commodity colo space, built to Uptime Institute Tier-3
standards.&lt;/p&gt;
&lt;p&gt;In that are mostly two-socket E5-type machines, and it&amp;rsquo;s at the
two extreme ends of the spectrum, E5-2620&amp;rsquo;s and E5-2690&amp;rsquo;s. We
would like to be able to run on the 2620 stuff completely, of
course, but some parts of our application require the clock and
the Oomph of the large CPUs. We are working on that.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.005-1.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;The part of the workload that actually earns us money is mostly
replacing strings taken from a database in HTML templates, we
are a REP MOVSB company. That&amp;rsquo;s boring work, and that is good.&lt;/p&gt;
&lt;p&gt;We try to keep stuff in memory where possible, especially where
customer facing machines are affected. No disk reads, and no
rotating storage in customer facing work.&lt;/p&gt;
&lt;p&gt;There is more complicated stuff in the background. We are
running the usual big data stuff with added deep learning on
top, in image processing, fraud detection and in experimental
user interfaces. We host a large scale data movement
infrastructure with MySQL, Elasticsearch, Cassandra and Hadoop
being involved.&lt;/p&gt;
&lt;p&gt;We are trying very hard to be customer centric. Technology is
seen as a necessity, but it&amp;rsquo;s forced upon us by scale. We&amp;rsquo;d
rather focus on the hotel thing. :-)&lt;/p&gt;
&lt;p&gt;A lot of stuff is recent, due to growth - Moore&amp;rsquo;s law also
applies backwards. Speaking about growth:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.006.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;There are a few old numbers taken from quarterly public company
performance reports, roomnights and hotels. But it could be
about any metric at Booking, servers, requests, people working
there, meals served in the canteen or sum of database requests -
you would see the same curve.&lt;/p&gt;
&lt;p&gt;On a log-Y scale it&amp;rsquo;s even nicer:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.007.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Some nice straight lines.If I were to draw the yearly increase
in compute power as given by Moores law into this graph, you
would see that this is also a mostly flat line, unfortunately
one that is raising far slower that the other lines here.&lt;/p&gt;
&lt;p&gt;For a scalability person like me it means a tough life.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.008.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Drawing the lines out naively sees us covering the earth in data
centers by 2040 or so. That&amp;rsquo;s a problem, because where are we
going to put the hotels?&lt;/p&gt;
&lt;p&gt;Of course that&amp;rsquo;s not going to happen, but it underlines that we
need to change our ways of handling stuff.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.009.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Currently, we are handling the hardware part &amp;ldquo;enterprise&amp;rsquo;y&amp;rdquo;. So
we get space, build out the room, add racks, switches and
chassis and then put hardware in.&lt;/p&gt;
&lt;p&gt;That happens in waves, with different latency and mutual
dependencies, and that can go wrong in many ways. On the other
hand, it delays decisions and spending as long as possible. To
us, being a very Dutch company with a very high volatility due
to the growth, delaying such Capex as long as possible is
attractive.&lt;/p&gt;
&lt;p&gt;Once the Hardware is down, we are on top of things - we have a
system of software components called ServerDB which basically
enables full hardware lifecycle management, interfacing at the
front with the Purchase Orders database, doing everything a user
could want to do to a machine with API and Web Interfaces, and
finalizing machine lifetime many times later with a
decommissioning workflow.&lt;/p&gt;
&lt;p&gt;ServerDB not only manages the hardware, it also has complete
overview about all other assets, does the config management for
switches, storage, collects power data and temperature
information and links into monitoring, load balancer
configuration and to puppet.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.010.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Using commercial colo space means many constraints. We are in
small rooms, multiple 0.5MW sizes, and that creates a number of
placement constraints and unwanted cabling requirements.&lt;/p&gt;
&lt;p&gt;The power and cooling framework we have to live in is about
7kW/rack, which is about half of what we want. Look at the
image, that&amp;rsquo;s a blade center that has the potential to create
6.4kW of power draw under full load, so you are looking at
non-oversubscribed racks that are 10/40U full. That&amp;rsquo;s about knee
high.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.011.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Here is a test run of a single blade from that bladecenter,
where I am exercising all the cores as hard as possible,
recording the power draw. We reach 50% power draw at 6/56 cores
busy, and using the maximum power draw, times 16 blades, gives
me a total load of 6.4kW from the whole enclosure.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.012.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Similarly, here is a rack of 32 machines (actually 40, but I got
only 32 due to BMC instabilities), exercising at full power.
These are hadoop units, and I can get them to consume 15kW in a
7kW environment, hotspotting and oversubscribing hard.&lt;/p&gt;
&lt;p&gt;Some constraints border on the ridiculous, but when we are
receiving a monthly delivery in individual parts, we spam the
unloading and docking areas of the data center with unpacking
trash, making the other tenants quite angry. Also, the build
times are getting unwieldy.&lt;/p&gt;
&lt;p&gt;Finally, the BMC, which is our gateway into the machinery for
ServerDB and it&amp;rsquo;s assorted tools, is a big problem.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.013.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;ServerDB contains an abstraction layer that tries to hide the
various differences in functionality and API between the various
kinds of machinery we have. We could do without that just fine,
and Redfish is actually a partial fix to that.&lt;/p&gt;
&lt;p&gt;Despite the fact that we do not use many features, still
ServerDB utilizes Redfish, native vendor specific LOM
functionality and ssh to get what it needs - Power control,
Reset, and setting boot preferences, optionally firmware update
and optionally partitioning and controller setup.&lt;/p&gt;
&lt;p&gt;We collect metrics: power, temperature, cooling and faults, but
that is purely read-only. Auditability is becoming more relevant&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;making config changes without a visible interruption of
production is actually arguably a disadvantage, and all the
recents security problems all the way down the stack are
worrying us: BMC, ME and Microcode issues now.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Much of that is underdocumented, and there are way to many way
too convenient ways to access this, from dedicated interfaces to
shared interfaces to local gateways when you physically on the
machine and have access to i2c buses or similar. The defaults
are often insecure, the crypto is often outdated, and the client
requirements are often insecure as well (&amp;ldquo;Install a Java version
your security department will hate you for having on your
machine.&amp;quot;).&lt;/p&gt;
&lt;p&gt;So where do we got from here? What is required to graduate from
this?&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.014.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Three issues need adressing: Going Rack Scale, Bringing volume
up and Getting Rooms to match. Let&amp;rsquo;s look at each of these in
turn.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.016.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;We started ordering hardware by the blade center chassis. That
did good: It solves many of the trash issues, it streamlines the
late provisioning phase where we are putting actual hardware
into the rack. It&amp;rsquo;s limited by what can go into a chassis, and
the chassis we are using are going to be discontinued.&lt;/p&gt;
&lt;p&gt;We understand why that is necessary, but the direction that is
going to is not what we want. We could do this more, and start
to order by the rack. That has a few requirements that I will be
adressing later, but if you are planning this, the choice is
basically between Intel Rack Scale Design derivatives or Open
Compute.&lt;/p&gt;
&lt;p&gt;It gives you more design flexibility, and it may or may not
retain the savings in power and cooling from shared
power/cooling setup depending on what you do.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.017.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;The Intel RSD solution is available in many vendor flavors, HP,
Dell and Supermicro all have them. It gives you a Pod and a Rack
controller not unlike a bladecenter chassis controller. On top
of the functionality that ServerDB already offers, you get a
chassis or rack-wide PCI bus, and composable hardware, where you
can build physical machine configurations from CPU and storage
modules in the rack.&lt;/p&gt;
&lt;p&gt;Some companies like Dell even offer casings around the
machinery, in the form of semi-transportable data centers.
Unfortunately most of these solutions try to minimise space and
maximise density, so they come with other constraints and design
decisions that we are reluctant to take on - basically you have
to live in units of 8x8x40 ft shipping containers, and that&amp;rsquo;s
rather limiting.&lt;/p&gt;
&lt;p&gt;It&amp;rsquo;s all nifty engineering, but all very vendor specific and
bespoke, when we are really looking for bulk hardware, all as
alike as possible, and with fewer features, not more. Having
&lt;em&gt;no&lt;/em&gt; &lt;em&gt;differentiator&lt;/em&gt; is actually a feature we are looking for.
Value add subtracts from the value from the value the machinery
has for us. The actual value add happens higher up the stack, in
software - in management for us that&amp;rsquo;s ServerDB and in
production for us that is more or less Kubernetes or something
that does what K8s does, but differently.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.018.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;And that&amp;rsquo;s exactly the sales promise from Open Compute. Clean,
bare server designs without value add. Documented interfaces
with source code available to read up on the details, so we can
integrate our stuff without problems. Delivery by the rack, and
on top of that, potential operational and capital savings, if
you treat room and rack as a system.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.019.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Getting the volume up is the next important consideration, and
if we are looking at our machine park, that&amp;rsquo;s going to be a
problem.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.020.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;If you go into ServerDB and count machine profiles, it goes up
to 11: blade 9 and two additional blade2 variants. Actually
digging into this yields basically &amp;lsquo;large and small&amp;rsquo; CPU configs
and memory configs, but lots of different local storage options.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.021.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;The answer is, to quote Jello Biafra, obvious: Ban Everything!
In our case, local storage. So we disaggregate:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.022.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;In a rack design where we give a 2 OU, 500 W, 100 GBit slot to
the storage people and provide un-RAID-ed 2 TB local SSD as the
only storage option, we have to build a network to match this.
We are anticipating more east-west traffic from storage
replication and east-west traffic from storage acess. All of
that is happening front-side, production, on regular TCP/IP.&lt;/p&gt;
&lt;p&gt;But that means I have only one network topology to scale and
maintain, and I have no longer placement constraints for
workloads: I can build uniform compute, and demand &amp;ldquo;no local
persistence&amp;rdquo;. If you wan to keep your MySQL datadir, put it on
iSCSI, RoCE or NVME via TCP and be done with it. It will be the
size and have the properties you require, it will be at least on
par with local SSD and it will still be there when your machine
dies and you are rescheduling to another spare machine elsewhere
in another rack.&lt;/p&gt;
&lt;p&gt;With Kubernetes on top, you get application mobility,
resiliency, and capacity size adjustment, which is fine, because
even the smallest Silver 4110 is going to be too large for most
units of deployment (in Java, consider 8 cores, 16 GB of RAM to
be a limit, for example).&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.023.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Being able to pull that off will give us 4 profiles or fewer,
disk made to measure as requested, location independence for
applications, and the ability to upgrade hardware without
interfering with the workload. I can evacuate a rack, do my
thing and put it back into service. Also, I can interchange the
components of my machinery independently: storage, memory,
compute and the networking parts are separate and can be on
different renewal cycles. All of that bound together by on
single TCP/IP network, not some bespoke PCI or FC/AL stuff.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.024.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Assuming I all have that, I will need rooms to match the rack in
order to fully leverage the advantages of Open Compute.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.025.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Open Compute hardware is built to be able to run in a barn:
concrete floor, air-in of up to 35 or 40 deg Centigrade,
hot-aisle containment, all service from the front because data
center operations engineers cannot survive in the hot aisle, and
we can do away with most of the equipment that is part of a
Tier-3 spec.&lt;/p&gt;
&lt;p&gt;It is being said that 1 MW of Tier-3 spec costs about 10 million
for the empty building shell, and that we can get OCP space
built for 2.5 Million/MW according to Facebook. I will be happy
if I can get space for half of the Tier-3 cost or less.&lt;/p&gt;
&lt;p&gt;Non of these savings will materialize if you put OCP into a
traditional Tier-3 building - the power path is not simplified,
the air-in is overcooled and the airco is overdesigned in order
to achive the overcooling which we do no longer require, so the
actual PUE is actually way worse than what we could have from an
OCP compliant data center.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.026.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;We do get Tier-3 space easily, though. That is because it is
being built without a buyers name on the constract. Buyers and
sellers meet later, and that is possible over the Tier-3 spec
from Uptime.&lt;/p&gt;
&lt;p&gt;The result is a two sided market, where demand and supply are
anonymous, unknown at the time the supply is being built.&lt;/p&gt;
&lt;p&gt;That is not possible at the moment with Open Compute, because
the spec is lacking, not well known to builders and the demand
is not worth building for it. If you need or want OCP compliant
space, that&amp;rsquo;s a larger commitment, because the space is being
built for you, and you will need to keep it for the building
lifetime.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.027.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;So having an OCP compliant data center build spec for OCP
hardware is important to make OCP attractive to smaller
deployments. They do need the quick availability and higher
flexibility of prebuilt space, because that&amp;rsquo;s lowering the
height of the commitment and provides fewer operative
distractions. There is risk for the space provider, so it&amp;rsquo;s
certainly higher cost than a data center build to spec for one
named entity, but that&amp;rsquo;s probably worth it. For a deployment our
size, there is still the problem of many 0.5MW rooms or similar,
capacity fragmentation, but for a more normal sized company
that&amp;rsquo;s actually a non-issue. They can be happy in these spaces.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2018/02/ocp.028.jpeg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;So to sum it up again: How can OCP work on non-hyperscaler
scales? Provide a two sided market for DC space over a shared
spec, then put hardware in there that matches what the room
provides. This creates operational and capital savings from
leveraging the room and the rack as a system that is mutually
dependent and built for each other.&lt;/p&gt;
&lt;p&gt;Then, once you have stuff in there, have an ecosystem that uses
the documented machine interfaces, all of them interchangeable
and interoperable. Have an ecosystem of open source management
modules like ServerDB for management and Kubernetes for
production, built on top of that, in order to bring utilisation
up and management cost down.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Be simple. Be boring. Be obvious.</title>
      <link>https://blog.koehntopp.info/2016/09/01/be-simple-be-boring-be-obvious.html</link>
      <pubDate>Thu, 01 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2016/09/01/be-simple-be-boring-be-obvious.html</guid>
      <description>&lt;p&gt;On Core.Infra day, I was invited to speak. This is my talk. There were many like it, but this was mine.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.001.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;h2 id=&#34;in-operations-code-is-not-your-friend&#34;&gt;
    &lt;a href=&#34;#in-operations-code-is-not-your-friend&#34;&gt;
	In operations, code is not your friend.
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This is about what I did the two years I have not been around here. I first worked for Booking in 2006 as a MySQL consultant doing databasey things. I joined Booking as an employee later, in 2008. By then, I was working in Amsterdam, but living in Berlin, so I was flying around quite a bit. In 2014, that became unworkable and I signed on with a Berlin based eCommerce hoster to do some awesome stuffs.&lt;/p&gt;
&lt;p&gt;SysEleven had about 3000 customers, mostly German eCommerce or news sites. The company had less then 50 employees in 2014 and grew to 70 people now. I joined a newly formed team, whose task was to build the new hosting platform for SysEleven.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.004.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Given that we were to build the new platform, there has to be an old one. The old platform was based on single machines with all storage local, and maybe a NetApp or two mounted from external. Hosting was done on OpenVZ based containers, and the Underlay (Hypervisor) was generally managed by puppet. The Overlay (contents of the container), too, mostly. But due to limitations in Puppet, architectural decisions of the past and the nature of the workload imposed by customers that was not done consistently and completely. There was a lot of fuzz around the edges, and some things were completely on manual.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.005.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;The new platform was based on Openstack, with a software designed storage solution, and a software designed network solution underneath, no local storage on the machine (all bytes always come from the SDS). Underlay and Overlay were to be completely controlled by Puppet, and in fact, the actual installation process was to be done by Puppet as well.&lt;/p&gt;
&lt;p&gt;We actually succeeded in doing this.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.006.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;So you have several racks full of HP DL380s, all powered off and cold. The bootstrap talks to the iLOs, turns on the hardware, inventories everything, zaps fresh ROMs and iLOs onto everything, and then paints a base operating system onto the machines that is just enough to bootstrap Puppet. Puppet then continues to install and personalise the machines and build an Openstack from this, including SDS and SDN cluster builds before the Openstack install.&lt;/p&gt;
&lt;p&gt;The total installation time was 30-45 minutes, if everything went well, but usually not everything went well. :-) Also, Puppet as a tool is extremely focused on managing a single host and does not work well with the concept of a cluster. If you want to build such a thing, you end up coding a lot of stuff against the internal logic of Puppet, using Stages to have synchronisation points, or invoking very ugly hacks written as external Python scripts that sync up the cluster before allowing everybody to continue to the next phase.&lt;/p&gt;
&lt;p&gt;I learned that there is a place that can genuinely and rightfully be called Puppet Hell.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.007.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;We made the decision to not write Puppet modules when we could reuse external work. That turned out to be a not so smart solution. We imported external repositories into our internal Gitlab, and in order to structure things, we set up a bunch of different segregated areas, teams. In the end we had about 200 repos, in six different teams.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.008.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;That led to a lot of problems. We did not set out to have these problems, it just kind of inevitably happened.&lt;/p&gt;
&lt;p&gt;200 Repos is quite a lot, and even with tooling you end up making mistakes. Some people will not be able to see some repos, not because they should not, but because of mistakes. Some things are done manually, others are done automatically, but rules change or team members move, but the rules are not adjusted and other stuff.&lt;/p&gt;
&lt;p&gt;Anyway, you inevitably end up in a situation where you are making a change and you are not aware of all the consumers of the change - either because you can’t see them, or because you did not bother to check out that repo, or because your search did not find them. Not fixing your consumers will break them, and hence the build. That is annoying.&lt;/p&gt;
&lt;p&gt;You also can’t make your changes atomically. You may make your change at the source, and merge it, but you still need to merge and push the 13 consumers of your change, and that can take time, or fail. Until you complete the merge in all Repos, nobody in the team can roll out. That sucks.&lt;/p&gt;
&lt;p&gt;Finally, if visibility is limited, it may be that two people are producing or importing duplicate solutions for a problem. Worse, it may be that these duplication are actually duplicate external imports, but at different version requirements. That makes it very hard to manage project dependencies and size.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.009.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;We are doing Puppet, so what is it that we import when we import stuff from Puppetlabs, or github?&lt;/p&gt;
&lt;p&gt;Well, mostly generalised configuration management classes for Puppet.&lt;/p&gt;
&lt;p&gt;What is that?&lt;/p&gt;
&lt;p&gt;Most configuration files are actually Hashes of Hashes with a unique kind of syntactic sugar. MySQL my.cnf files, Bind configuration, Openstack ini-Files, and so on - the are all HoH’s.&lt;/p&gt;
&lt;p&gt;So what you get is a YAML-shovel that will generate the HoH that is, say, an Apache vHost-definition from the Hash of Hashes inside Puppet variables. Which in turn are being loaded from the Hash of Hashes that is a Hiera YAML file.&lt;/p&gt;
&lt;p&gt;When you want to make a config change, you can’t edit a native Apache config file. You need to read the code, seeing how the Puppet in-memory HoH is being transformed into the actual Apache config. Then you need to look further back and understand how the twelve dozen Hiera files actually overlay each other to form these Puppet structures, and inject your change in the right place.&lt;/p&gt;
&lt;p&gt;Then you need to try this out and actually see what config is being generated.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.010.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;You need a lab. For a config change.&lt;/p&gt;
&lt;p&gt;That is because your config and your config generation became so complicated that it is no longer obvious. It is no longer obvious what that code does, and it is no longer obvious what your change does.&lt;/p&gt;
&lt;p&gt;In our case the test environment is an entire datacenter, the vanguard data center. That is clearly insane.&lt;/p&gt;
&lt;p&gt;How did we end up in this place?&lt;/p&gt;
&lt;p&gt;Let me introduce &lt;a href=&#34;http://mikehadlow.blogspot.com/2012/05/configuration-complexity-clock.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Configuration Complexity Clock&lt;/a&gt;

.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.011.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;You write a piece of actual code, in a proper programming language. Values are pieces of code, constants. They are baked in. It is noon on the config clock.&lt;/p&gt;
&lt;p&gt;Things change, values need to change. You need different values in different places, so you load variables from a file, the config file. It is now 3.&lt;/p&gt;
&lt;p&gt;Somebody needs more than that - there are rules. “Some rooms are only available at some price if the stay includes a Saturday” is such a rule. Or “The amount of memory we give to the Innodb buffer pool is all machine memory minus 20% or 4GB, whichever is smaller, unless it’s a mz box with all that Blob stuff going on.” is another such rule. It is now 6 on the config clock.&lt;/p&gt;
&lt;p&gt;Of course some things can no longer be expressed as rules. You need a fully blown language with data structures, control structures, loops and include files. You get a config management DSL. Something like puppet. It’s 9 on the config clock.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.012.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;»The team spend most of their time writing code in the new DSL.&lt;/p&gt;
&lt;p&gt;After some embarrassing episodes, they now go through a complete release cycle before deploying new DSL code.&lt;/p&gt;
&lt;p&gt;The DSL text files are version controlled and each release goes through regression testing before being deployed.«&lt;/p&gt;
&lt;p&gt;That’s SysEleven, doing puppet. That’s us, doing Puppet - if not now, then it’s going to be us very soon.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.013.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;That is, because what we are doing is no longer obvious, so we need to check the presence and type of parameters to our Puppet classes.&lt;/p&gt;
&lt;p&gt;We need to write tests that check the validity and content of the subconfigs generated.&lt;/p&gt;
&lt;p&gt;We then need to do integration testing, in order to prevent catastrophic loss of the entire site from centralised config changes.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.014.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;At SysEleven, when we were at that point, we stopped. We threw away our complete stack of “Setup Openstack from scratch” and started over.&lt;/p&gt;
&lt;p&gt;We chose a different config management system, in our case that was Ansible, but that is not really important.&lt;/p&gt;
&lt;p&gt;We did choose something else than Puppet, mainly so that we were unable to directly reuse any component we previously wrote.&lt;/p&gt;
&lt;p&gt;We did come up with some rules in order to prevent ourselves from descending into Puppet Hell again. Our goal was to enforce simplicity, and obviousness.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.015.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;We did switch to a Monorepo, at least as much as possible. There were other, external dependencies with other parts of the company that prevented us from going completely Monorepo, but it kind of worked well enough for us.&lt;/p&gt;
&lt;p&gt;We did use Ansible, and while the actual choice of the config management system officially did not matter as long as it is not Puppet, Ansible does have a number of advantages that Puppet does not have. One of them being that it deals much better with distributed software than Puppet can do, and the other, much more important one being that it is very limited.&lt;/p&gt;
&lt;p&gt;It is impossible to do complicated things in Ansible. At some point, if Ansible is not sufficient, you need to stop ansibilizing and switch to Python. That’s a good point to raise the issue with the team and asking around what’s actually the problem and how an Ansible extension written in Python is going to make this better.&lt;/p&gt;
&lt;p&gt;This is good, because it gives you a point where reflection and goal-setting are in order, and a hook where project management processes can be invoked.&lt;/p&gt;
&lt;p&gt;We also looked at our YAML shovels and asked ourselves what they are good for. We found that they are overgeneralised solutions for our problems. We do not need to be able to write any conceivable Apache config from our config management code, we only need to be able to write our Apache config from it.&lt;/p&gt;
&lt;p&gt;Or, more generally:&lt;/p&gt;
&lt;h2 id=&#34;feature-development-is-about-generalisation-avoiding-limits-delaying-decisions-and-being-more-flexible&#34;&gt;
    &lt;a href=&#34;#feature-development-is-about-generalisation-avoiding-limits-delaying-decisions-and-being-more-flexible&#34;&gt;
	Feature development is about generalisation, avoiding limits, delaying decisions and being more flexible.
    &lt;/a&gt;
&lt;/h2&gt;
&lt;h2 id=&#34;infrastructure-development-is-about-being-concrete-making-decisions-setting-limits-and-being-obvious&#34;&gt;
    &lt;a href=&#34;#infrastructure-development-is-about-being-concrete-making-decisions-setting-limits-and-being-obvious&#34;&gt;
	Infrastructure development is about being concrete, making decisions, setting limits and being obvious.
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;I cannot stress the importance of that enough. It was at the core of all of our problems.&lt;/p&gt;
&lt;p&gt;We decided to write config files directly. If there is an Apache config, write that Apache config file in Apache config file syntax into the Ansible template directory, and install it as literally as possible. Changes will be obvious in what they do.&lt;/p&gt;
&lt;p&gt;Do not use variables, unless you can show that in our config there is actually variation.&lt;/p&gt;
&lt;p&gt;We did catch ourselves in review in anticipating problems we didn’t have, introducing complexity we didn’t need. Good thing we did establish review, because every single team member failed at being as simple as possible, multiple times.&lt;/p&gt;
&lt;p&gt;We did have a few cases were it was in order to write code, remove duplication and generate stuff in a loop or similar things, but these cases were few and far between.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.016.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Our new setup was an order of magnitiude smaller and much, much easier to understand.&lt;/p&gt;
&lt;p&gt;We did bring in external people and tried to explain to them what we did, and how things work, and the new setup was so much easier to work with that this was actually possible.&lt;/p&gt;
&lt;p&gt;We did not need tests any more, because changes were obvious. Code that is not present does not need to be tested in the first place.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2016/09/obvious/obvious.017.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;So, looking at us here: What’s in it for us?&lt;/p&gt;
&lt;p&gt;I do believe, after reading a lot of code in our Puppet and in other places, that Simplicity as a value in itself would work well for us, too.  Code isn’t our friend, either, especially in Core.Infra and Operations related places. We are here to make decisions, not delay them, and to be obvious, and as simple as we can get away with. Which is hard enough given the fact that our size and growth force more and more distributed stuff on us, which by it’s very nature has a high inherent complexity.&lt;/p&gt;
&lt;p&gt;I think the entire container thing is a brilliant idea, and we should be doing it more.&lt;/p&gt;
&lt;p&gt;A lot more.&lt;/p&gt;
&lt;p&gt;Immutable containers, being created and then loading values from a Zookeeper or Consul, instead of running Puppet on every host, painting state changes over state changes that may or may not have been previously applied.&lt;/p&gt;
&lt;p&gt;Eventually we will no longer have any need for Puppet at all any more, despite the fact that we are a larger company that is doing a lot more distributed stuff than now.&lt;/p&gt;
&lt;h2 id=&#34;the-tao-of-operations&#34;&gt;
    &lt;a href=&#34;#the-tao-of-operations&#34;&gt;
	The Tao of Operations:
    &lt;/a&gt;
&lt;/h2&gt;
&lt;h3 id=&#34;be-simple-be-boring-be-obvious&#34;&gt;
    &lt;a href=&#34;#be-simple-be-boring-be-obvious&#34;&gt;
	Be simple. Be boring. Be obvious.
    &lt;/a&gt;
&lt;/h3&gt;
&lt;h3 id=&#34;because-every-time-you-arent-you-get-to-write-a-postmortem&#34;&gt;
    &lt;a href=&#34;#because-every-time-you-arent-you-get-to-write-a-postmortem&#34;&gt;
	Because every time you aren’t, you get to write a postmortem.
    &lt;/a&gt;
&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Go away, or I will replace you with a very small shell script</title>
      <link>https://blog.koehntopp.info/2015/03/27/go-away-or-i-will-replace-you.html</link>
      <pubDate>Fri, 27 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://blog.koehntopp.info/2015/03/27/go-away-or-i-will-replace-you.html</guid>
      <description>&lt;p&gt;This is the writeup for the english variant of the talk &amp;ldquo;Go away or I will replace you with a very small Shell script&amp;rdquo;. The original version of the talk was given in German at &lt;a href=&#34;https://guug.de/veranstaltungen/ffg2015/abstracts.html#3_1_1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GUUG FFG 2015&lt;/a&gt;

 in Stuttgart. &lt;a href=&#34;https://www.youtube.com/watch?v=e0CCv7pSK4s&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A recorded version in german language&lt;/a&gt;

 has been made at Froscon in August 2015.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.001.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Go Away Or I Will Replace You With A Very Small Shell Script or There Is No Such Thing As A Devops Team&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I came up with this talk, because I was invited to the GUUG FFG with the ask &amp;ldquo;to give some thoughts about Devops&amp;rdquo;. I ended up with something that in some way is a reflection about what changed in how we do computers, between approximately the years 2000 and 2010.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.005.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;At Devops.com, Rajat Bhargava tries to explain in &lt;a href=&#34;https://devops.com/devops_and-enterprises/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DevOps and Enterprises: It&amp;rsquo;s a culture thing&lt;/a&gt;

 that&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;DevOps is about increasing company performance through better IT execution. The hypothesis is that by more closely aligning what cus with what gets built in a more timely fashion, organizations will sell more products and services. That’s not a small company or SMB issue, that’s an every company issue.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In general, change in companies does not happen by itself - companies by construction try to repeatedly execute the same thing with the same results over and over. Change only happens because of outside pressures. So if companies change the way they do IT to &amp;ldquo;increase company performance through better IT execution&amp;rdquo;, there are outside influences at work that exert pressure on the processes, forcing change.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at these outside influences.&lt;/p&gt;
&lt;h2 id=&#34;maturity-of-market-and-process-and-growth-matters&#34;&gt;
    &lt;a href=&#34;#maturity-of-market-and-process-and-growth-matters&#34;&gt;
	Maturity of Market and Process, and Growth matters
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.012.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A maturing environment (left hand side) allowed the development and establishment of quantitatively governed processes. After reaching a certain process maturity level (right hand side), outsourcing of processes in full or in part becomes possible, enabling a make-or-buy decision at the management level.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The Cynefin model is a soft thing, a model or frame of mind that is used by people to have words and categories to express what tools and project structures make sense in a given context: It can be used to classify &amp;ldquo;decision making contexts&amp;rdquo;. A decision making context is for example a market in which a company is going to act, and then it would become a way to express &amp;ldquo;market maturity&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;If you apply it to &amp;ldquo;the development of the early Internet&amp;rdquo; or &amp;ldquo;the Dotcom bubble before 2000&amp;rdquo;, then you see the very early Dotcom bubble as &amp;ldquo;Nobody has a clue how to even make money on the Internet, much less how to structure a company around this idea&amp;rdquo;. This is the Chaotic domain, subject of the &amp;ldquo;Research&amp;rdquo; model, in which you try out things, and see what sticks. You do not know the rules at all, not even if they are static or changing. You need to get any structure at all. The work model is &amp;ldquo;Act&amp;rdquo; - try a thing, &amp;ldquo;Sense&amp;rdquo; - see what happens and &amp;ldquo;Respond&amp;rdquo; - discard the attempt or if it works, try to modify it and see what changes. With &amp;ldquo;Novel&amp;rdquo; practice you work with very small teams of highly qualified people - a research team - and try to establish and formulate some ground rules. You expect to lose money and to throw away many experiments, quickly.&lt;/p&gt;
&lt;p&gt;Once you have found a working business model, and successfully extract any money at all, you can switch to the &amp;ldquo;Complex&amp;rdquo; domain and build on your emergent practice. You secure the position you have found and try out small things, instead of wild guesses all over the place. Again, you try to register what that does to your business and improve (instead of trying something else entirely). Basically you have the business model and now try to build practice at all. You try to move things from Research to Engineering. This also changes team structure - you would work with your Researchers moving to Lead positions with a bunch of applied sciences or engineering grade people working for them. This is now multiple small teams at work, on different aspects of incremental improvement. You focus on iteration, small improvements and may experiment with some basic automation. In the Complex you know there are rules, but they may be not easily discoverable, and they may be changing and stateful.&lt;/p&gt;
&lt;p&gt;As the market and the business matures, it may move to the &amp;ldquo;Complicated&amp;rdquo; domain, in which a Business Model and Basic Practice exists and you now try to find dimensions along which to judge these practices (&amp;ldquo;What are the metrics?&amp;quot;) and then try to establish measuring processes for this. Using the metrics you can sense, and exert control by analyzing the metrics and how they change. This is no longer research at all, but engineering at the verge of pure business excecution, preparing for scaleout. You remove the researchers, and add engineering bureaucrats that establish the framework for quantification. In the Complicated market rules exist, and are discoverable, and usually are static or the rules by which they change are also known.&lt;/p&gt;
&lt;p&gt;In the Complex and especially in the Complicated, companies are growing in a growing market, and focus is not so much on efficiency as on land grab. Management says things like &amp;ldquo;Do not spend more than we earn at any time, but first and foremost, do not stall growth.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Finally things move into &amp;ldquo;Simple&amp;rdquo; or &amp;ldquo;Obvious&amp;rdquo; domain, which has well established procedural handbooks, metrics and can draw on past iterations that are similar to the current. So you measure, sense, then look up the proper response and execute it. Best Practice and quantified metrics exist, and you can scale out the process by hiring and training moderately trained people working from the book you prepared in the previous step. This is a mature and stable market, with obvious and discoverable rules and playbooks for all situations. In a mature market, growth is usually only possible at the cost of others, as the market itself is not growing and there is no unclaimed or untapped market share. Instead, growth often is created by efficieny improvements. This is the domain of bureaucrats and beancounters.&lt;/p&gt;
&lt;p&gt;At the right hand side, we have a process maturity model - here it is &lt;a href=&#34;https://en.wikipedia.org/wiki/Capability_Maturity_Model_Integration&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CMMI&lt;/a&gt;

, but any will do, really. The point being that process maturity follows market maturity: Establishing mature, quantified processes means that you need to be able to iterate, and iterations need to be comparable. So this is basically impossible in the Chaotic and Complex, and also not in the parts of the Complicated where the company is still growing rapidly. If you experiment internally, or if you grow by 10x every other year, your processes will change, even if the deliverables stay the same. That also means that establishing comparable metrics on process execution is impossible.&lt;/p&gt;
&lt;p&gt;For outsourcing decisions that means they are impossible. In order to have agreement on what is being bought and what has been delivered, it is necessary to have metrics (and before that, a taxonomy). Only then it is possible for both parties to agree what the product is and if it has been delivered as agreed. That also means that Pre-Obvious or Fast-Growing companies will bias towards insourcing, whereas companies where efficiency matters can make the Make-or-Buy decision and decide to outsource.&lt;/p&gt;
&lt;h2 id=&#34;scale-up-vs-scale-out&#34;&gt;
    &lt;a href=&#34;#scale-up-vs-scale-out&#34;&gt;
	Scale-Up vs. Scale-Out
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.013.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;At the beginning of the Dotcom bubble, running IT mostly meant Enterprise IT. Some somebody somewhere had a data center, and in the data center was a large computer called server. Somebody else now connected the data center network to the Internet. After figuring out a few basic concepts such as Firewalls, DMZ and basic network security, people started about thinking connecting the internal IT to the Internet. At that time the Web came around.&lt;/p&gt;
&lt;p&gt;As the Web grew, more people used it, and the load on the existing large computers grew. What to do?&lt;/p&gt;
&lt;p&gt;Obviously: buy a larger, faster and newer computer. &lt;a href=&#34;https://en.wikipedia.org/wiki/Moore%27s_law&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Moore&amp;rsquo;s Law&lt;/a&gt;

 gives you an annual growth of around 45% year on year, and that should be right, shouldn&amp;rsquo;t it?&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.014.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;One of twelve Enterprise 10000 &amp;ldquo;Starfire&amp;rdquo; at mobilcom, Germany, around 2002.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;We see some seriously large machines at around that time - there were twelve 64 CPU Enterprise 10000 &amp;ldquo;Starfire&amp;rdquo; machines present at the mobilcom Data Center in Germany in the early 2000&amp;rsquo;s, for example, and many more were deployed in other places just in the area I worked in.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.015.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Yet it became obvious rather quickly that this was not an approach that worked, and even if it worked, it often was prohibitively complex, expensive and slow. Once that became clear, people started to automate basic tasks, learned to mange large fleets of moderately sized machines and then tried a different approach - &amp;ldquo;scale out&amp;rdquo; instead of &amp;ldquo;scale up&amp;rdquo;. Use more computers instead of larger computers.&lt;/p&gt;
&lt;p&gt;This triggered a lot of learnings at all levels of the profession - network, data center structure and operations practice needed revision. This talk will focus on the latter, but the other topics are also of interest.&lt;/p&gt;
&lt;h2 id=&#34;operations-change-in-response-to-scale-out&#34;&gt;
    &lt;a href=&#34;#operations-change-in-response-to-scale-out&#34;&gt;
	Operations change in response to scale-out
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.016.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;I have been known to annoy people with saying that &amp;ldquo;Whenever you are using ssh, you might as well open a ticket.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;If you need to login to a box manually to look something up, you are looking at a monitoring defect - make a ticket for that. If you need to login to a box manually to change something, you are looking at an automation defect - make a ticket for that. I have literally been beaten for saying that.&lt;/p&gt;
&lt;p&gt;We get cfengine 3 (2004), and later Puppet (2005), Salt (2011) and Ansible (2012). Also, somewhere at the end of the observed time frame virtualisation becomes viable, and with that we get automated provisioning of hardware and &amp;ldquo;Infrastructure in Code&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.017.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;The thinking that permeates software development can now be applied to hardware and infrastructure resources: If there is a problem with a setup, recreate that setup, and the problem in a branch. Then modify the branch to see if that fixes the problem. When it does, merge the branch, and redeploy automatically.&lt;/p&gt;
&lt;p&gt;We get alignment of tooling in operations with tooling in development. This, and &amp;ldquo;zero manual interactions&amp;rdquo; grade of automation have a great impact:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.018.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Automating things makes them reproducible: The same change applied to many instances always produces the same outcome.&lt;/p&gt;
&lt;p&gt;If that actually works, and has zero manual interactions, we can apply the change to the entire fleet in parallel, instead of filtering it through a single person and serializing it that way.&lt;/p&gt;
&lt;p&gt;Once you can do things fully automated and in parallel, it does no longer matter so much if a single thing fails - you do not care about individual instances any more. Instead you care more about always having sufficient capacity in general, and how to orchestrate all your instances so that this condition is never ever violated.&lt;/p&gt;
&lt;p&gt;It is this stage where we see people moving from &amp;ldquo;highly available setups&amp;rdquo; using Pacemaker and Active/Passive pairs to loadbalancers with &lt;em&gt;n&lt;/em&gt; workers, all of them active at a sufficiently low utilization factor to buffer away a loss of &lt;em&gt;m&lt;/em&gt; units of capacity. All computing becomes distributed computing at some point - more about that in another talk.&lt;/p&gt;
&lt;h2 id=&#34;this-requires-a-different-mindset-and-different-qualifications&#34;&gt;
    &lt;a href=&#34;#this-requires-a-different-mindset-and-different-qualifications&#34;&gt;
	This requires a different mindset, and different qualifications
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.019.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;When we change the tooling and the methods to be more closely aligned with Developers, culture also has to change.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.020.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;A figure of the pre-2000 USENET sysadmin culture has been the BOFH - the &lt;a href=&#34;http://bofh.bjash.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bastard Operator From Hell&lt;/a&gt;

. These are the stories about an outright hostile systems operator, written by Simon Traviaglia and posted to USENET. Parts of that have been sold to magazines and printed later on.&lt;/p&gt;
&lt;p&gt;A lot of people picked up on the concept, and the USENET groups alt.sysadmin.recovery and de.alt.sysadmin.recovery were born. People met, for example in German at the series of Cannossa parties.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.021.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;This is not a good mindset or culture. It not only rejects development, and change per se, it is also toxic and hostile, even to the people who actually pay the bill. Never a good idea.&lt;/p&gt;
&lt;p&gt;Of course it is a joke, or satire, but unfortunately, stories influcence minds, even when they are meant to be funny or satire.&lt;/p&gt;
&lt;p&gt;Around the same time, Thomas A. Limoncelli wrote the proto-devops book &amp;ldquo;The practice of System Administration&amp;rdquo;, which interestingly contained a chapter on how not be to a BOFH - it asked the system administrator to reflect on the structure and processes of the larger company and their place in them. Limoncelli then goes on about how to build useful supporting and reporting structures to function successfully in that structure. He closes with advice on how to respond to unexpected demands without too much toil and effort. It foreshadows a lot of development that caught on later under the label devops.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.022.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;The current, updated edition of that book is titled &amp;ldquo;The Practice of Systems and network Administration&amp;rdquo;.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The term &amp;ldquo;Devops&amp;rdquo; meanwhile, was coined by &lt;a href=&#34;https://twitter.com/patrickdebois&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Patric Debois&lt;/a&gt;

 in 2008 in Belgium, and pretty much assembled and then taught the same ideas as the Limoncelli book, at a larger scale and using even clearer structure.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.023.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&lt;em&gt;BOFH practice applied in the 2008 Booking.com office (re-enactment).&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;so-how-do-dev-and-ops-differ&#34;&gt;
    &lt;a href=&#34;#so-how-do-dev-and-ops-differ&#34;&gt;
	So how do Dev and Ops differ?
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Well, even at 2008 we already have processes for IT organisations. Large, scaled up and unwieldy processes that have been established top down, though: During the Dotcom boom, the structure of IT support and operations processes had been formulated as well in &lt;a href=&#34;https://en.wikipedia.org/wiki/ITIL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ITIL&lt;/a&gt;

, and then often badly implemented in the wild.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.024.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;It is entirely valid and sometimes helpful to think of Devops as a reaction to bad ITIL deployments:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.025.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Bottom up, &amp;ldquo;apply development methods and thinking to IT operations, while staying small and agile and iterating quickly, and also focusing on automation, metrics capture and data driven improvement&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Tear down the wall between software development and operations. Teach operators coding and coding thinking, and teach developers to care about operations and how operations matter, how scale matters.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.026.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Jez Humble describes this tweet as a reaction:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In a fit of rage caused by reading yet another email in which one of our customers proposed creating a &amp;ldquo;devops team&amp;rdquo; so as to &amp;ldquo;implement&amp;rdquo; devops, I tweeted that &amp;ldquo;THERE IS NO SUCH THING AS A DEVOPS TEAM.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and later wrote &lt;a href=&#34;https://continuousdelivery.com/2012/10/theres-no-such-thing-as-a-devops-team/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;a longer blog post&lt;/a&gt;

 about this, the core of the argument being&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Devops movement addresses the dysfunction that results from organizations composed of functional silos. Thus, creating another functional silo that sits between dev and ops is clearly a poor (and ironic) way to try and solve these problems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Applying developer methods to operations problems changes the environment and tooling. The modern stack looks like this:&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.033.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Systems are provisioned with &amp;ldquo;Infrastructure as Code&amp;rdquo;, automatically and on demand, via an API with no humans involved.&lt;/li&gt;
&lt;li&gt;The code for the application and the infrastructure itself resides in a shared version control system, where it can also be subject to automation.&lt;/li&gt;
&lt;li&gt;There is a one step build and deploy automation that produces deliverables. These are then automatically deployed into production with zero humans involved, if so desired.&lt;/li&gt;
&lt;li&gt;The deployment and the activation of code pathes is separated, using feature flags and instrumentation to compare performance aspects of these code pathes. Separation of deployment and activation is key to safe and fast rollouts, gradual activation and &lt;a href=&#34;https://blog.koehntopp.info/2020/01/17/rolling-back-a-rollout.html&#34;&gt;one-click rollbacks&lt;/a&gt;

.&lt;/li&gt;
&lt;li&gt;Monitoring and freshness indicator on monitoring metrics are key to proper failure detection.&lt;/li&gt;
&lt;li&gt;Instant and shared communication is key to decisive join action in failure situations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are two fundamental innovations in here that deserve a special highlight from a &amp;ldquo;ten years later&amp;rdquo; perspective:&lt;/p&gt;
&lt;h3 id=&#34;separation-of-rollout-and-activation&#34;&gt;
    &lt;a href=&#34;#separation-of-rollout-and-activation&#34;&gt;
	Separation of rollout and activation
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;When writing new code, it is useful to wrap the new code and the old code being replaced by the new code into two branches of a feature flag. This is because in a large enough deployment you cannot migrate to the new code in a single atomic transaction anyway: old and new are going to inevitably co-exist for some time.&lt;/p&gt;
&lt;p&gt;But if that is the case, you might as well make the most of it: Separating rollout and activation allows you to roll out code into production without running it at all, and without running it for all users. Instead you can choose who gets exposed to the new variant in a very controlled way: You can run it for one user, for 1% of the user base, for Mac users only or for users coming from IP addresses identifing as Japanese in origin, or any combination of such criteria.&lt;/p&gt;
&lt;p&gt;You can also instrument that code, and then compare how it behaves - in conversion of sales, in cost of execution, in execution speed and along many dimensions more.&lt;/p&gt;
&lt;p&gt;Separation of rollout and activation, and proper instrumentation of the wrapped code allow you to experiment with variants in production.&lt;/p&gt;
&lt;h3 id=&#34;observability-through-metrics-from-events&#34;&gt;
    &lt;a href=&#34;#observability-through-metrics-from-events&#34;&gt;
	Observability through Metrics from Events
    &lt;/a&gt;
&lt;/h3&gt;
&lt;p&gt;Another thing that has proven to be useful is to build a monitoring system on top of an event system. Instead of collecting numeric metrics on machines and pre-aggregating them, logging structured data (JSON or similar) and collecting them centrally has advantages: The numeric metrics can be extracted from the events and pre-aggregated for common and known things to report on, but since the actual events constituting the metrics are persisted, other important things become possible:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Metrics can tie back to the actual events that make up the numbers. So when you have identified an anomaly in space (the machines involved) and time, you can look at the actual events that make up the numbers you looked at, and try to identify a root cause.&lt;/li&gt;
&lt;li&gt;Since the actual raw events are available after the fact, it is possible to correlate things over time: &amp;ldquo;When we changed the upper sales funnel this way, immediate conversion increased, but effectively we lost money, because of increased return rates and customer support cost in a three week window after each sale, for &lt;em&gt;x&lt;/em&gt; percent of the customers of the B-variant of the experiment&amp;rdquo;. These are findings that are impossible to prove without access to raw event data.&lt;/li&gt;
&lt;li&gt;Since the actual raw events are available, it is possible to query the raw event data in other, previously unknown dimensions, searching for additional patters - it is possible to debug using event data instead of going each box &amp;ldquo;in person&amp;rdquo; (with ssh) and observing things on the ground.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;a-clash-of-cultures&#34;&gt;
    &lt;a href=&#34;#a-clash-of-cultures&#34;&gt;
	A clash of cultures
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.039.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;In any case: We get convergence in tooling between Dev and Ops.&lt;/p&gt;
&lt;p&gt;Operational people learn to code, and use this to automate operations, centralize and standardize the monitoring, and developers learn to care about operational aspects, building operations support directly into software.&lt;/p&gt;
&lt;p&gt;Eventually, developers and admins are using the same set of tools. So, let&amp;rsquo;s converge the teams. That&amp;rsquo;s Devops.&lt;/p&gt;
&lt;h2 id=&#34;its-not-that-simple-and-doesnt-work&#34;&gt;
    &lt;a href=&#34;#its-not-that-simple-and-doesnt-work&#34;&gt;
	It&amp;rsquo;s not that simple and doesn&amp;rsquo;t work?
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.040.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Inevitably, cultures clash.&lt;/p&gt;
&lt;p&gt;Despite the same projects and identical tooling, there are critical differences.&lt;/p&gt;
&lt;p&gt;But they seem to be harder to spot.&lt;/p&gt;
&lt;p&gt;What are they about?&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.041.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Feature-Developers (&amp;ldquo;Developers&amp;rdquo;) and Infrastructure-Developers (former &amp;ldquo;Operations&amp;rdquo; people) seem to have utterly different metrics for success.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.042.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Infrastructure developers see feature developers as people who focus on new best cases: There is a plan, there is sprint. New features - new best cases - are developed, and what is deemed finished is being released in a big showy party. Everybody is colorful, happy and rolling across the lawn. What isn&amp;rsquo;t finished goes back onto the backlog, and that is that.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.043.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Infrastructure developer know that is not true for themselves. They look at themselves like in this picture, complete with helmet webcams.&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Nobody has ever flipped a light switch and exclaimed &amp;lsquo;Awesome. The light actually turned on!&amp;rsquo; when it worked. But flip the switch only once, and it does not turn on: people will complain and remember that for a long time.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;Because of that, infrastructure developers judge change by looking at how worst cases behave and how worst case behavior changes with changed code. Only then they will look at other improvements.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.046.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;There are two famous Booking rules for rollouts, from the early days:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;#1 If you break it, will you even notice?&lt;/li&gt;
&lt;li&gt;#2 If you break it, can you fix it?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The answer to #1 should be &amp;ldquo;Yes!&amp;rdquo;, of course. But that means you need to know your dependencies and dependents, your place in the larger scope of things. If also means you need to know what people your change will affect, and how to find them and to synchronize with them. If you do not know these things, you do not know how your change will affect these people, and that means you cannot safely roll out.&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s okay, we have training for that, and we can help you with that.&lt;/p&gt;
&lt;p&gt;The answer to #2 should be &amp;ldquo;Yes!&amp;rdquo;, again, or if cannot be that, it should be &amp;ldquo;but I know who can, and I made sure they are aware of my change and available for help&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;So both of these rules are dealing with failure - anticipating and handling it properly. It&amp;rsquo;s infrastructure developer thinking, but we give these rules to feature developers. That&amp;rsquo;s a mind hack.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.048.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Infrastructure thinking is really hard to explain to outsiders.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.049.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;For example, in this document, the Linux &amp;ldquo;Code of Conflict&amp;rdquo;, what is being tried is to explain this - &amp;ldquo;There are people who will read your code, and they will evaluate it on how it fails, and how it changes failure before they even look at what it improves, because that is how Infrastructure works. Criticizing your code is not criticizing you, listen and learn.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;It also explains: &amp;ldquo;Criticizing you instead of your code is not okay, so if that happens, call out for help, please.&amp;rdquo; It uses many words for that, because somehow most non-infrastructure people are not used to this.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.051.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;There is a certain type of experience associated with this kind of mindset. &amp;ldquo;Who here remembers this one?&amp;rdquo; When I asked the original audience of the talk about this incident, about 2/3 of the audience raised their hands. &amp;ldquo;I was there!&amp;rdquo; was being shouted by a few people.&lt;/p&gt;
&lt;p&gt;That was at that point in time 14 years ago.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.052.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Ten years earlier, at Friday, the 13. May of 2005, I had to shut down 2MW of compute at the web.de data center, turning off the email for 25 million customers, because of a complete loss of cooling in the Data Center. We handled the incident, turned off the entire data center in less than 20 minutes in a chaotic rumble, and then back on to basic functionality with another two hours of work. Fully redundant and properly configured setup was reached again on Saturday, the 14th, around noon.&lt;/p&gt;
&lt;p&gt;People in the audience also remembered that one.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.053.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Of course, Infrastructure developers are averse to change. Change introduces unknown behavior, and unknown operating conditions.&lt;/p&gt;
&lt;p&gt;Of course, if you are always ever judged by your failures, you focus on failure cases and how they are handled.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.054.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;This does not have to be a contradiction, though. A sequence of rapid small changes acutally makes deployment risk smaller, and allows you to fail safely in many cases - in all cases, even, with a bit of engineering and good practice - see above, the discussion about events, monitoring and separation of rollout and activation.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.056.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Planning for change and budgeting downtime can help a lot, even. Internally, we teach this as shown in the slides below: We have a failure budget, in lost potential income, and we expect to make use of it, even.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.057.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;We look at what happened in a blameless postmortem process, and then write down what the takeaways from the outage are - how do we need to change processes, how to we need to improve monitoring and training, how do we need to fix code in order for that failure and that class of failure to go away?&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.058.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;We talk about the concept of &amp;ldquo;Careful Carelessness&amp;rdquo;, which is what allows you to jump out of a plane several thousand meters up in the air, more than once.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.060.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;The &amp;ldquo;Careful&amp;rdquo; part of skydiving is important. You need training, you need to know and trust your buddies, you need a plan for safe landing, and you need alternative plans, tested and ready, and reviewed by others. You need to review their plans.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.064.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;The core concept is to make changes &amp;ldquo;survivable&amp;rdquo;, and then be able to execute the entire process in a way that it can be done often, without hurting. What is a project elsewhere, a one-off thing with additional staffing and a deadline, is a process for us, routinely, repeatedly done as part of normal operations, all of the time.&lt;/p&gt;
&lt;p&gt;This is what Devops is about: Making change a routine process of everyday operations.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.069.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Survivability&amp;rdquo; means to fail, or almost fail, but live to walk away and tell the tale. Because that is how we learn: When we succeed, we only confirm what we already know. When we fail, or almost fail, we learn a new thing, and we can share that experience with our peers.&lt;/p&gt;
&lt;p&gt;Not only do we learn from failure, we also share common history and experience, and that builds better communication, validates judgement, and builds trust. This is how you forge a team.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.071.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Testing in production is okay, if you engineer for it, and make it survivable. Even outside of rollouts, you can increase resilience by introducing chaos and variabilty in procedures - don&amp;rsquo;t shut down systems cleanly, always jank them out of production, or even install the chaos monkey. Always test in production, and find ways to do this safely. Also, this will help you find dependencies and metrics that matter.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.076.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Martin Seeger of NetUSE is famous for popularizing the proverb &amp;ldquo;Nobody wants Backup. Everybody needs Restore.&amp;rdquo; He wants to highlight the fact that Backups are just a cost center, and do not produce anything of value. The value - which needs to be proven - is in the successful Restore, and that is also what needs to be tested, constantly.&lt;/p&gt;
&lt;p&gt;Of course, if you automate, you do not need backups for anything besides the (clearly defined and isolated) state. You can rebuild your systems at will, in regular intervals, or for testing purposes, and then inject the actually unique state into them.&lt;/p&gt;
&lt;h2 id=&#34;and-yet-they-are-still-fighting&#34;&gt;
    &lt;a href=&#34;#and-yet-they-are-still-fighting&#34;&gt;
	And yet, they are still fighting
    &lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.077.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;And yet, Dev and Ops are still fighting.&lt;/p&gt;
&lt;p&gt;Why is that?&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.078.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Developers tend to ignore operational complexity and toil, and often build from building blocks that look like rectangles on an architecture diagram, but are actually complex systems in themselves.&lt;/p&gt;
&lt;p&gt;This is Openstack Monasca, Monitoring as a Service:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;Monasca is an open-source multi-tenant, highly scalable, performant, fault-tolerant monitoring-as-a-service solution that integrates with OpenStack. It uses a REST API for high-speed metrics processing and querying, and has a streaming alarm engine and a notification engine.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&amp;ldquo;It uses a number of underlying technologies; Apache Kafka, Apache Storm, Zookeeper, MySQL, Vagrant, Dropwizard, InfluxDB and Vertica.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;At which point somebody in the audience usually shouts &amp;ldquo;Bingo!&amp;rdquo;. There are a number of questions here - for example, &amp;ldquo;How do you hire for this?&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.083.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Or, looking at this Openstack Infrastructure Diagram (simplified): How do you operate this under &amp;ldquo;system behavior in failure state&amp;rdquo; as a success metric - which is how Infrastructure people think.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.087.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Infrastructure people see code like this coming in, and see people who package things they have not understood, taking on dependencies they do not know, and using practices that look like automated, repeatable procedures, but aren&amp;rsquo;t.&lt;/p&gt;
&lt;p&gt;The first line installs Homebrew - &amp;ldquo;Download an unreviewed script from Github and feed it to a shell, executing random foreign commands on your system&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The second example shows a Dockerfile executing, but what looks like a build procedure is really just downloading binaries in tar files from elsewhere, unpacking and piling them on top of each other in a badly specified binary patch procedure, without caring much what is in these packages and how it is being made.&lt;/p&gt;
&lt;p&gt;The last example is part of an Openstack Puppet install, and downloads an actual operating system package, then NOT installing it, but unpacking it and copying individual unregistered files into a production system image.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.089.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Or implementations of upgrade procedures with three nested loops (O(n^3) complexity), that work for l,m and n = 1 on a test laptop, but cannot possibly succeed in any production environment with significant values of l,m or n - clearly this has never seen an actual production environment.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.090.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;This captures the essence of this mindset, cargo culting, containerism. Abstractions packed away deep in a fragile stack, and then in production suddenly breaking, taking down the entire technology Jenga tower.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.091.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;Computer science is weird - it is hard, despite the fact that it is the science of zeroes and ones. Nothing individually in computer science is ever hard. It literally is as simple as Jenga or Tetris, and like these, the complexity comes from the layers.&lt;/p&gt;
&lt;p&gt;I have an exercise where I let people list the dependencies for their application going down from the business level all the way down to the silicon, and then we count layers stacked on top of each other. We usually can identify about three dozen levels of abstractions being piled on top of each other, all of them trivial or almost trivial. And that is on an isolated system, keeping the vagaries of distributed systems out of the picture.&lt;/p&gt;
&lt;p&gt;So when we think about complexity in computer science, we speak about epsilon-delta in non-linear systems with cascading dependencies: A tiny change here has catastrophic outcomes elsewhere, 20 layers up or down the stack. You add a line of code, the working set of your application no longer fits into the CPU cache of low end CPU models, and suddenly the performance difference between the same code running on a Silver or Gold Xeon model is factor 20 and nobody even knows why.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.096.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;This is not a new complaint. Alan Perlis famously quipped this about LISP programmers in 1982: The level of abstraction in LISP in 1982 was so high that native LISP programmers with no insight into the implementation created similar situations to the previous cache scenario, regularly.&lt;/p&gt;
&lt;p&gt;&lt;p class=&#34;md__image&#34;&gt;
  &lt;img src=&#34;https://blog.koehntopp.info/uploads/2015/03/devops-en.099.jpg&#34; alt=&#34;&#34;  /&gt;
&lt;/p&gt;

&lt;/p&gt;
&lt;p&gt;In closing: What changes?&lt;/p&gt;
&lt;p&gt;Devops means that Operations people become Infrastructure Developers. They are using the same tools that Feature Developers are using, but for a different purpose: For creating, scaling, maintaining and debugging the production environment of a project. For dealing with all the real world, failure related use-cases of a project.&lt;/p&gt;
&lt;p&gt;Devops means that the System Administrator as a job description goes away: There are only Infrastructure Developers now, or Operators - but these are people that follow instructions and will be ultimately replaced by machines soon.&lt;/p&gt;
&lt;p&gt;Devops means that as a System Administrator you have to learn the tools of a Developer, learn to automate, learn to talk to APIs, and learn how to apply Infrastructure thinking to other peoples codebases.&lt;/p&gt;
&lt;p&gt;As a Devops Engineer, or Infrastructure Developer, it is your task to keep the entire stack in mind. Applying Infrastructure thinking to this means you know how that change 20 levels down in the stack affects production on a grander scale - because somebody has to know the details and understand all the dependencies.&lt;/p&gt;
&lt;p&gt;As a Devops Engineer you also need to teach enthusiastic young people with a feature developer mindset how to &lt;a href=&#34;https://blog.koehntopp.info/2020/08/31/on-touching-candles.html&#34;&gt;touch candles&lt;/a&gt;

, how to fail safely, in order to make them actually experience a problem class, instead of just abstractly knowing about it somewhere in the back of their mind.&lt;/p&gt;
&lt;p&gt;If you do not do that, if you cannot do that, you will soon be replaced by a tiny shell script.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

