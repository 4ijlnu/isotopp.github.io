<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>mysqldev on Die wunderbare Welt von Isotopp</title>
    <link>/tags/mysqldev.html</link>
    <description>Recent content in mysqldev on Die wunderbare Welt von Isotopp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/mysqldev/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MySQL: Python and WHERE ... IN ()</title>
      <link>/2021/10/28/python-where-in.html</link>
      <pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/10/28/python-where-in.html</guid>
      <description>As a developer using Python, I want to be able to hand a list to an SQL statement with a WHERE id IN (…) clause, and it should do the right thing.
 Well, that is not how it started, because it was asked on the internal no-work-channel, so it kind of escalated more.
A question   The original question was:
 Dev&amp;gt; Why is it 2021, and SQL prepared statements still can&amp;rsquo;t deal with IN?</description>
    </item>
    
    <item>
      <title>MySQL: Our MySQL in 2010, a hiring interview question</title>
      <link>/2021/09/27/mysql-booking-2010-a-hiring-interview-question.html</link>
      <pubDate>Mon, 27 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/09/27/mysql-booking-2010-a-hiring-interview-question.html</guid>
      <description>I ranted about hiring interviews, and the canned questions that people have to answer. One of the interviews we do is a systems design interview, where we want to see how (senior) people use components and patterns to design a system for reliability and scale-out.
A sample question (based on a Twitter thread in German):
 It is 2010, and the company has a database structure where a fixed number front end machines form a cell.</description>
    </item>
    
    <item>
      <title>MySQL: Binding the ORM</title>
      <link>/2021/09/17/binding-the-orm.html</link>
      <pubDate>Fri, 17 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/09/17/binding-the-orm.html</guid>
      <description>My task is to collect performance data about a single query, using PERFORMANCE_SCHEMA (P_S for short) in MySQL, to ship it elsewhere for integration with other data.
In a grander scheme of things, I will need to define what performance data from a query I am actually interested in. I will also need to find a way to attribute the query (as seen on the server) to a point in the codebase of the client, which is not always easy when an ORM or other SQL generator is being used.</description>
    </item>
    
    <item>
      <title>MySQL: Tracing a single query with PERFORMANCE_SCHEMA</title>
      <link>/2021/09/15/mysql-tracing-a-single-query-with-performanceschema.html</link>
      <pubDate>Wed, 15 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/09/15/mysql-tracing-a-single-query-with-performanceschema.html</guid>
      <description>My task is to collect performance data about a single query, using PERFORMANCE_SCHEMA (P_S for short) in MySQL, to ship it elsewhere for integration with other data.
In a grander scheme of things, I will need to define what performance data from a query I am actually interested in. I will also need to find a way to attribute the query (as seen on the server) to a point in the codebase of the client, which is not always easy when an ORM or other SQL generator is being used.</description>
    </item>
    
    <item>
      <title>MySQL: Page compression revisited</title>
      <link>/2021/09/14/mysql-page-compression-revisited.html</link>
      <pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/09/14/mysql-page-compression-revisited.html</guid>
      <description>Like I said, I never had much reason to use table compression, and only recently looked into the topic. MySQL Page Compression looks a lot easier at the database end of things, but relies on hole punching support in the file system. Let&amp;rsquo;s have a look at what that means.
Files, Inodes and Arrays of Blocks   The original Unix filesystem saw the disk as a sea of blocks, which were represented in a free map as an array of bits.</description>
    </item>
    
    <item>
      <title>MySQL: CREATE IF NOT EXISTS TABLE, but CREATE OR REPLACE VIEW</title>
      <link>/2021/09/10/create-if-not-exists.html</link>
      <pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/09/10/create-if-not-exists.html</guid>
      <description>For the MySQL Million Challenge, I was going through the server syntax in order to understand what things can be created in the server. And now my OCD triggered. DDL is a mess.
Creation    As a database developer, I want to be able to create server objects using the CREATE thing syntax.
 The server gives you that for the following things:
 DATABASE EVENT FUNCTION (and FUNCTION SONAME) INDEX LOGFILE GROUP (NDB only, not going to look at this) PROCEDURE RESOURCE GROUP ROLE SERVER SPATIAL REFERENCE SYSTEM TABLE TABLESPACE TRIGGER USER VIEW  Safe creation    As a database developer I want to be able to script things safely, so I need IF NOT EXISTS clauses in my CREATE syntax.</description>
    </item>
    
    <item>
      <title>MySQL: The Million Challenge</title>
      <link>/2021/09/10/mysql-million-challenge.html</link>
      <pubDate>Fri, 10 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/09/10/mysql-million-challenge.html</guid>
      <description>A long-standing idea that I have is to test the servers limits: How does it fail and break if there are very many of a thing? Previously that was too easy, because many structures were constructed in a way that it was obvious they would not scale. But with MySQL 8 many things were overhauled, so let&amp;rsquo;s see what we can make many of and see how the server fares.</description>
    </item>
    
    <item>
      <title>MySQL: The table &#39;../tmp/#sql…&#39; is full</title>
      <link>/2021/09/09/tmp-file-full.html</link>
      <pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/09/09/tmp-file-full.html</guid>
      <description>We observe a large number of messages of the kind
The table &amp;#39;../tmp/#sql…&amp;#39; is full Before MySQL 8   In older Versions of MySQL, implied temporary tables are being created, whenever your EXPLAIN contained the phrase using temporary.
In this case, MySQL would create an in-memory temporary table to materialize an intermediate query result, and then continue to process the data from there. If that temporary table was larger than some configurable limit, the temporary table would instead be converted to a MyISAM table on disk, streamed out, and then work would continue with this.</description>
    </item>
    
    <item>
      <title>MySQL: Two kinds of compression</title>
      <link>/2021/09/09/mysql-two-kinds-of-compression.html</link>
      <pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/09/09/mysql-two-kinds-of-compression.html</guid>
      <description>I never had much reason to use table compression, so I largely ignored the topic in MySQL. I knew that MySQL had table compression since 5.1, but I also knew the implementation was horribly complicated and double stored all data. There is also page compression, a feature introduced with 5.7, which replaces table compression and works much better.
Table Compression   Table Compression is available in MySQL 5.1 and newer.</description>
    </item>
    
    <item>
      <title>MySQL: A job queue in Python</title>
      <link>/2021/07/13/mysql-a-job-queue-in-python.html</link>
      <pubDate>Tue, 13 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/07/13/mysql-a-job-queue-in-python.html</guid>
      <description>Somebody needed a job queue in Python: Multiple writers insert into it in random order, and the jobs are written into the MySQL table jobs. From the jobs table, multiple consumers claim jobs in batches of n or smaller (n=100), and process them. After processing, the consumers delete the jobs. We need concurrent job generation and consumption, with proper and efficient locking.
The full source for this example can be seen in mysql-dev-examples in mysql-claim-jobs.</description>
    </item>
    
    <item>
      <title>Database as a Queue</title>
      <link>/2021/01/28/database-as-a-queue.html</link>
      <pubDate>Thu, 28 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/01/28/database-as-a-queue.html</guid>
      <description>The DBA experience at work suggests that every single schema at some point in its lifecycle holds a queue table. These are tables in which some processes (the “producers”) put rows, which a swarm of other processes (the “consumers”) lock and consume.
A variation on that theme is the state machine, in which jobs are placed by producers. Consumers do not immediately delete them, but update them a few times to indicate processing progress, before the rows are ultimately being deleted.</description>
    </item>
    
    <item>
      <title>SQL Clause is coming to town</title>
      <link>/2020/12/26/sql-clause-is-coming-to-town.html</link>
      <pubDate>Sat, 26 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/12/26/sql-clause-is-coming-to-town.html</guid>
      <description>Olya Kudriavtseva has an ugly christmas sweater :
 &amp;ldquo;He&amp;rsquo;s making a table. He&amp;rsquo;s sorting it twice. SELECT * FROM contacts WHERE behavior = &amp;ldquo;nice&amp;rdquo;; SQL Clause is coming town! (buy here )
Katie Bauer observes :
 I mean, except for the fact that sorting something twice is TERRIBLY optimized
 So how bad is this? Let&amp;rsquo;s find out.
Some test data   We are defining a table santa, where we store peoples names (GDPR, EU Regulation 2016/679 applies!</description>
    </item>
    
    <item>
      <title>Backups and Replication</title>
      <link>/2020/11/27/backups-and-replication.html</link>
      <pubDate>Fri, 27 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/11/27/backups-and-replication.html</guid>
      <description>There was a question at work about MySQL backups and restore. I needed to explain more.
We use databases to make state persistent. That is: As a developer you can think of your database as a single giant, structured global variable with a weird access method, and to make things worse, concurrent access.
 A database is just a global variable to your code
 We can log statements that change the state of our database in a log.</description>
    </item>
    
    <item>
      <title>An unexpected pool size increase</title>
      <link>/2020/10/07/an-unexpeced-pool-size-increase.html</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/07/an-unexpeced-pool-size-increase.html</guid>
      <description>At work, replication chains have a single primary database node, to which you write, and then multiple replicas, in multiple AZs.
Here is what the one sample chain looks like in Orchestrator:
instance-918d is the current primary, in the blue AZ. Replicas in orange and green are in other AZs. Blue badges indicate multiple replicas, eg (38) means 38 machines.
When you talk to a database, you get two database handles:</description>
    </item>
    
    <item>
      <title>MySQL: Import CSV, not using LOAD DATA</title>
      <link>/2020/09/28/mysql-import-csv-not-using-load-data.html</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/28/mysql-import-csv-not-using-load-data.html</guid>
      <description>All over the Internet people are having trouble getting LOAD DATA and LOAD DATA LOCAL to work. Frankly, do not use them, and especially not the LOCAL variant. They are insecure, and even if you get them to work, they are limited and unlikely to do what you want. Write a small data load program as shown below.
Not using LOAD DATA LOCAL   The fine manual says :</description>
    </item>
    
    <item>
      <title>Importing account statements and building a data warehouse</title>
      <link>/2020/09/26/my-private-data-warehouse.html</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/26/my-private-data-warehouse.html</guid>
      <description>This is an update and translation of a much older article , which I wrote in German Language back then. I was experimenting with importing the account statements from my German Sparkasse, which at that time were being made available as a CSV.
The initial data load   The data looked like this:
$ head -2 /home/kris/Documents/banking/umsatz-22758031-29122004.csv &amp;#34;Local Account&amp;#34;;&amp;#34;Book Date&amp;#34;;&amp;#34;Valuta Date&amp;#34;;&amp;#34;Transaction Type&amp;#34;; &amp;#34;Purpose&amp;#34;; &amp;#34;Remote Party&amp;#34;;&amp;#34;Remote Account&amp;#34;;&amp;#34;Bank Code&amp;#34;; &amp;#34;Amount&amp;#34;;&amp;#34;Currency&amp;#34;;&amp;#34;Info&amp;#34; &amp;#34;08154711&amp;#34;;&amp;#34;30.12&amp;#34;;&amp;#34;30.12.05&amp;#34;;&amp;#34;Direct Debit&amp;#34;; &amp;#34;DRP 08154711 040441777 INKL.</description>
    </item>
    
    <item>
      <title>MySQL: automatic partitions surely would be nice</title>
      <link>/2020/09/25/mysql-dynamic-partitions-suck.html</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/25/mysql-dynamic-partitions-suck.html</guid>
      <description>In Deleting data we have been looking at a process that loads data into MySQL, leveraging partitions to make it easier and faster to later get rid of the data again. For this, we created three processes, a data loader process, and two observers - one for creating partitions, and one for deleting them.
The observer processes have been running ANALYZE TABLES and then polling INFORMATION_SCHEMA.PARTITIONS every 1/10th of a second to check if intervention is needed.</description>
    </item>
    
    <item>
      <title>MySQL: Deleting data</title>
      <link>/2020/09/24/mysql-deleting-data.html</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/24/mysql-deleting-data.html</guid>
      <description>Completing the data lifecycle is often harder than originally expected: Deleting data can cost sometimes way more than inserting it in the first place. MySQL Partitions can offer a way out. We have an earlier post on the subject.
A sample table, and a problem statement   Let&amp;rsquo;s define a kind of log table, to which data is added with an auto_increment id value and some data.
#! /usr/bin/env python3 from time import sleep from random import randint from multiprocessing import Process import click import MySQLdb import MySQLdb.</description>
    </item>
    
    <item>
      <title>MySQL: Provisioning .mylogin.cnf</title>
      <link>/2020/09/23/mylogin-cnf.html</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/23/mylogin-cnf.html</guid>
      <description>MySQL uses connection and config parameters from a number of possible sources. The easiest way to find out where it is looking for config files is to run
$ mysql --help | grep cnf order of preference, my.cnf, $MYSQL_TCP_PORT, /etc/my.cnf /etc/mysql/my.cnf /Users/kkoehntopp/homebrew/etc/my.cnf ~/.my.cnf As can be seen, my version of the MySQL client checks in this order
 /etc/my.cnf /etc/mysql/my.cnf /Users/kkoehntopp/homebrew/etc/my/cnf ~/.my.cnf  The cnf file is a file in dot-ini syntax, so you have [groups] and each group contains lines with key = value pairs.</description>
    </item>
    
    <item>
      <title>MySQL: ALTER TABLE for UUID</title>
      <link>/2020/09/22/alter-table-for-uuid.html</link>
      <pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/22/alter-table-for-uuid.html</guid>
      <description>A question to the internal #DBA channel at work: »Is it possible to change a column type from BIGINT to VARCHAR ? Will the numbers be converted into a string version of the number or will be it a byte-wise transition that will screw the values?«
Further asking yielded more information: »The use-case is to have strings, to have UUIDs.«
So we have two questions to answer:
 Is ALTER TABLE t CHANGE COLUMN c lossy?</description>
    </item>
    
    <item>
      <title>MySQL: Encoding fields for great profit.</title>
      <link>/2020/09/18/mysql-encoding-fields-for-great-profit.html</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/18/mysql-encoding-fields-for-great-profit.html</guid>
      <description>Iterating schemas over time is not an uncommon thing. Often requirements emerge only after you have data, and then directed action is possible. Consequently, working on existing data, and structuring and cleaning it up is a common task.
In todays example we work with a log table that logged state transitions of things in freeform VARCHAR fields. After some time the log table grew quite sizeable, and the log strings are repeated rather often, contributing to the overall size of the table considerably.</description>
    </item>
    
    <item>
      <title>MySQL from a Developers Perspective</title>
      <link>/2020/09/07/mysql-from-a-developers-perspective.html</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/07/mysql-from-a-developers-perspective.html</guid>
      <description>So this has turned into a small series, explaining how to work with MYSQL from a developers perspective. This post is intended as a directory for the individual articles. It will be amended and re-dated as necessary.
The code for the series is also available in isotopp/mysql-dev-examples on GitHub.
The Tag #mysqldev will reference all articles from this series.
  MySQL Transactions - the physical side . Looking at how MySQL InnoDB handles transactions on the physical media, enabling rollback and commit.</description>
    </item>
    
    <item>
      <title>MySQL: Generated Columns and virtual indexes</title>
      <link>/2020/09/07/mysql-generated-columns-and-virtual-indexes.html</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/07/mysql-generated-columns-and-virtual-indexes.html</guid>
      <description>We have had a look at how MySQL 8 handles JSON recently, but with all those JSON functions and expressions it is clear that many JSON accesses cannot be fast. To grab data from a JSON column, you will use a lot of $-&amp;gt;&amp;gt;field expressions and similar, and without indexes nothing of this will be fast.
JSON cannot be indexed.
But MySQL 8 offers another feature that comes in handy: Generated columns and indexes on those.</description>
    </item>
    
    <item>
      <title>MySQL: Basic usage of the JSON data type</title>
      <link>/2020/09/04/mysql-json-data-type.html</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/04/mysql-json-data-type.html</guid>
      <description>MySQL 8 provides solid support for the JSON data type. The manual has an overview of the data type , a JSON function reference , an an overview on generated column indexes , and explains multi-values indexes .
Creating JSON columns   Creating JSON columns is easy: Make the column of the JSON data type, fill in valid JSON data.
mysql&amp;gt;createtablet(idintegernotnullprimarykeyauto_increment,jjson);QueryOK,0rowsaffected(0.11sec)mysql&amp;gt;insertintot(j)values-&amp;gt;(&amp;#39;null&amp;#39;),-&amp;gt;(&amp;#39;true&amp;#39;),-&amp;gt;(&amp;#39;false&amp;#39;),-&amp;gt;(&amp;#39;1&amp;#39;),-&amp;gt;(&amp;#39;&amp;#34;keks&amp;#34;&amp;#39;),-&amp;gt;(&amp;#39;[&amp;#34;eins&amp;#34;, &amp;#34;zwei&amp;#34;]&amp;#39;),-&amp;gt;(&amp;#39;{&amp;#34;eins&amp;#34;: &amp;#34;one&amp;#34;, &amp;#34;zwei&amp;#34;: &amp;#34;two&amp;#34;}&amp;#39;);QueryOK,5rowsaffected(0.02sec)mysql&amp;gt;selectjson_type(j)astype,json_valid(j)asvalid,isnull(j)assqlnull,j,idfromt;+---------+-------+---------+--------------------------------+----+ |type|valid|sqlnull|j|id|+---------+-------+---------+--------------------------------+----+ |NULL|NULL|1|NULL|1||NULL|1|0|null|2||BOOLEAN|1|0|true|3||BOOLEAN|1|0|false|4||INTEGER|1|0|1|5||STRING|1|0|&amp;#34;keks&amp;#34;|6||ARRAY|1|0|[&amp;#34;eins&amp;#34;,&amp;#34;zwei&amp;#34;]|7||OBJECT|1|0|{&amp;#34;eins&amp;#34;:&amp;#34;one&amp;#34;,&amp;#34;zwei&amp;#34;:&amp;#34;two&amp;#34;}|8|+---------+-------+---------+--------------------------------+----+ 8rowsinset(0.00sec)mysql&amp;gt;insertintot(j)values(&amp;#39;[&amp;#34;incomplete&amp;#34;, &amp;#34;array&amp;#34;, &amp;#34;closing bracket&amp;#34;&amp;#39;);ERROR3140(22032):InvalidJSONtext:&amp;#34;Missing a comma or &amp;#39;]&amp;#39; after an array element.</description>
    </item>
    
    <item>
      <title>MySQL: NULL is NULL</title>
      <link>/2020/08/25/null-is-null.html</link>
      <pubDate>Tue, 25 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/25/null-is-null.html</guid>
      <description>Question: Hey, I got a UNIQUE INDEX, but I can store multiple rows with the same value, NULL. That is surprising. Is that a bug?
 This is a rewrite of the same in German from 9 years ago .
 root@localhost[kris]&amp;gt;createtablet(ainteger,binteger,unique(a,b));QueryOK,0rowsaffected(0.09sec)root@localhost[kris]&amp;gt;insertintotvalues(1,2);QueryOK,1rowaffected(0.01sec)root@localhost[kris]&amp;gt;insertintotvalues(1,2);ERROR1062(23000):Duplicateentry&amp;#39;1-2&amp;#39;forkey&amp;#39;t.a&amp;#39;This does not work, as expected. But this does:
root@localhost[kris]&amp;gt;truncatetablet;QueryOK,0rowsaffected(0.16sec)root@localhost[kris]&amp;gt;insertintotvalues(1,NULL);QueryOK,1rowaffected(0.02sec)root@localhost[kris]&amp;gt;insertintotvalues(1,NULL);QueryOK,1rowaffected(0.03sec)root@localhost[kris]&amp;gt;select*fromt;+------+------+ |a|b|+------+------+ |1|NULL||1|NULL|+------+------+ 2rowsinset(0.00sec)Why is that?
This is usually where I point people at SQL for Smarties: Advanced SQL Programming .</description>
    </item>
    
    <item>
      <title>MySQL: Some Character Set Basics</title>
      <link>/2020/08/18/mysql-character-sets.html</link>
      <pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/18/mysql-character-sets.html</guid>
      <description>This is the updated and english version of some older posts of mine in German. It is likely still incomplete, and will need information added to match current MySQL, but hopefully it is already useful.
Old source articles in German: 1 , 2 and 3 .
Some vocabulary   Symbol, Font, Encoding and Collation - what do they even mean?
A character set is a collection of symbols that belong together.</description>
    </item>
    
    <item>
      <title>MySQL Foreign Key Constraints and Locking</title>
      <link>/2020/08/04/mysql-foreign-key-constraints-and-locking.html</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/04/mysql-foreign-key-constraints-and-locking.html</guid>
      <description>Since we now know how to look at the state of locking in a live database, let&amp;rsquo;s look at what happens when we run a normal insert or update and an insert or update with foreign key relationships defined, and compare.
We will be using the tables and structures from our previous examples, a simple 1:n relationship between a and b:
CREATETABLEa(a_idintNOTNULLAUTO_INCREMENT,PRIMARYKEY(a_id));INSERTINTOaVALUES(10),(20),(30),(40);CREATETABLEb(b_idintNOTNULLAUTO_INCREMENT,a_idintNOTNULL,PRIMARYKEY(b_id),KEY`a_id`(a_id),CONSTRAINTa_id_existsFOREIGNKEY(a_id)REFERENCESa(a_id)ONDELETERESTRICTONUPDATERESTRICT);INSERTINTObVALUES(10,10),(40,40);or the same definition for b without the constraint.</description>
    </item>
    
    <item>
      <title>MySQL Foreign Keys and Foreign Key Constraints</title>
      <link>/2020/08/03/mysql-foreign-keys-and-foreign-key-constraints.html</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/03/mysql-foreign-keys-and-foreign-key-constraints.html</guid>
      <description>Foreign Keys are what links tables together and turns a set of tables into a model. Foreign Key Constraints are conditions that must be true for the content of the tables to be an internally consistent model. Foreign Key Constraints can be defined and enforced in InnoDB, but this comes at a considerable price, and for some it may hurt more than it is worth.
A very simple shop as a ER-model.</description>
    </item>
    
    <item>
      <title>MySQL Deadlocks with INSERT</title>
      <link>/2020/08/02/mysql-deadlocks-with-insert.html</link>
      <pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/02/mysql-deadlocks-with-insert.html</guid>
      <description>Support Channel. &amp;ldquo;Hi, I am getting deadlocks in the database and they occur when I have to rollback the transactions but if we don&amp;rsquo;t have to roll back all transactions get executed.&amp;rdquo; Wait, what? After some back and forth it becomes clear that the Dev experiences deadlocks and has data:
mysql&amp;gt;pagerlessmysql&amp;gt;showengineinnodbstatus\G...MySQLthreadid142531,OSthreadhandle139990258222848,queryid4799571somehost.somedomainsomeuserupdateINSERTintosometable(identifier_id,currency,balance)VALUES(&amp;#39;d4e84cb1-4d56-4d67-9d16-1d548fd26b55&amp;#39;,&amp;#39;EUR&amp;#39;,&amp;#39;0&amp;#39;)***(2)HOLDSTHELOCK(S):RECORDLOCKSspaceid3523pageno1106463nbits224indexPRIMARYoftable`somedb`.`sometable`trxid9843342279lockmodeSlocksgapbeforerecand that is weird because of the lock mode S locks gap in the last line. We get the exact same statement with the exact same value on the second thread, but with lock mode X locks gap.</description>
    </item>
    
    <item>
      <title>MySQL: Locks and Deadlocks</title>
      <link>/2020/08/01/mysql-locks-and-deadlocks.html</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/01/mysql-locks-and-deadlocks.html</guid>
      <description>In a previous article we wrote data to the database using atomic update statements, and then using transactions with SELECT ... FOR UPDATE. In this article we will look at what happens when we continue doing this, in a more complicated way. Source code for this article is also available on github.com .
A simple row lock   But first let&amp;rsquo;s do things manually: We create a table kris with an integer primary key column and a secondary unindexed data column.</description>
    </item>
    
    <item>
      <title>MySQL Transactions - writing data</title>
      <link>/2020/07/30/mysql-transactions-writing-data.html</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/30/mysql-transactions-writing-data.html</guid>
      <description>Using the framework for testing we created in earlier articles, let&amp;rsquo;s try to modify some data. We are writing a small program that increments a counter. Our table looks like this, and contains 10 counters:
CREATETABLE`demo`(`id`bigintunsignedNOTNULLAUTO_INCREMENT,`counter`intNOTNULLDEFAULT&amp;#39;0&amp;#39;,UNIQUEKEY`id`(`id`))INSERTINTO`demo`VALUES(1,0);INSERTINTO`demo`VALUES(2,0);...INSERTINTO`demo`VALUES(10,0);We are using some very simple programming to increment a counter:
@sql.command() @click.option(&amp;#34;--name&amp;#34;, default=&amp;#34;demo&amp;#34;, help=&amp;#34;Table name to count in&amp;#34;) @click.option(&amp;#34;--id&amp;#34;, default=0, help=&amp;#34;Counter to use&amp;#34;) @click.option(&amp;#34;--count&amp;#34;, default=1000, help=&amp;#34;Number of increments&amp;#34;) def count(name, id, count): &amp;#34;&amp;#34;&amp;#34; Increment counter --id by --count many steps in table --name &amp;#34;&amp;#34;&amp;#34; for i in range(0, count): cmd = f&amp;#34;update {name}set counter=counter+1 where id = {id}&amp;#34; c = db.</description>
    </item>
    
    <item>
      <title>MySQL Transactions - the logical side</title>
      <link>/2020/07/29/mysql-transactions-the-logical-view.html</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/29/mysql-transactions-the-logical-view.html</guid>
      <description>After having a look how MySQL handles transactions physically , let&amp;rsquo;s have a look at what is going on from a logical point of view.
We are using a test table called demo with an id and a counter field, both integer. In it we have 10 counters, all set to 0.
CREATETABLE`demo`(`id`bigintunsignedNOTNULLAUTO_INCREMENT,`counter`intNOTNULLDEFAULT&amp;#39;0&amp;#39;,UNIQUEKEY`id`(`id`))INSERTINTO`demo`VALUES(1,0);INSERTINTO`demo`VALUES(2,0);...INSERTINTO`demo`VALUES(10,0);In one session, we start a transaction and modify a counter value. We do not commit anything.
Session1&amp;gt;starttransactionreadwrite;Session1&amp;gt;updatedemosetcounter=10whereid=3;Isolation   In a second session, we check the data and notice a few things:</description>
    </item>
    
    <item>
      <title>MySQL Connection Scoped State</title>
      <link>/2020/07/28/mysql-connection-scoped-state.html</link>
      <pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/28/mysql-connection-scoped-state.html</guid>
      <description>MySQL speaks its own proprietary protocol. It cannot be routed by a HTTP proxy, and a MySQL connection is entire unlike a HTTP connection. Specifically, a lot of state and configuration is tied to a MySQL connection, and it cannot be recovered on disconnect.
What state is tied to a connection?   Transactions   A disconnect implies a ROLLBACK. So if you are in a transaction, all changes to the database that you attempted are lost, rolled back, as if they never happened.</description>
    </item>
    
    <item>
      <title>MySQL Commit Size and Speed</title>
      <link>/2020/07/27/mysql-commit-size-and-speed.html</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/27/mysql-commit-size-and-speed.html</guid>
      <description>When writing data to disk, for small transactions the cost of writing the commit out do disk dominates the execution time of the script. In order to show that, I wrote a little bit of Python.
The script creates a test table in a database and writes 10.000 rows of test data into it, in commit sizes of 1, 2, 4, &amp;hellip;, 1024 rows.
$ ./mysql.py --help Usage: mysql.py [OPTIONS] COMMAND [ARGS].</description>
    </item>
    
    <item>
      <title>MySQL Transactions - the physical side</title>
      <link>/2020/07/27/mysql-transactions.html</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/27/mysql-transactions.html</guid>
      <description>So you talk to a database, doing transactions. What happens actually, behind the scenes? Let’s have a look.
There is a test table and we write data into it inside a transaction:
CREATETABLEt(idserial,datavarbinary(255))STARTTRANSACTIONREADWRITEINSERTINTOt(id,data)VALUES(NULL,RANDOM_BYTES(255))COMMITThe MySQL test instance we are talking to is running on a Linux machine, and otherwise idle to make observation easier. Also, we configured it with innodb_use_native_aio = false because observing actual physical asynchronous I/O and attributing it to the statement that caused it is really hard.</description>
    </item>
    
  </channel>
</rss>
