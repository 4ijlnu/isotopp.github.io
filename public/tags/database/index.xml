<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>database on Die wunderbare Welt von Isotopp</title>
    <link>/tags/database.html</link>
    <description>Recent content in database on Die wunderbare Welt von Isotopp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/database/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MySQL from Below</title>
      <link>/2021/02/25/mysql-from-below.html</link>
      <pubDate>Thu, 25 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/02/25/mysql-from-below.html</guid>
      <description>When you insert data into a database and run COMMIT you expect things to be there: Atomically, Consistent, Isolated and Durable , like Codd commanded us 40 years ago, but also quickly. There is a surprising amount of sophistication being poured into this, but since I do not want to shame MongoDB and Redis developers in this post, I am not going to talk about that much in this place.</description>
    </item>
    
    <item>
      <title>Validating storage</title>
      <link>/2021/02/24/validating-storage.html</link>
      <pubDate>Wed, 24 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>/2021/02/24/validating-storage.html</guid>
      <description>Where I work, we try to run databases in a memory saturated way. That is, we try to provide so much memory that the working set of the database is memory resident, or in other words, the number of disk reads after an initial warmup is no longer dependent on the database load.
Workload Intelligence Analytics showing &amp;ldquo;IOPS over time&amp;rdquo; for a mixed read/write benchmark on Datera iSCSI.
We can validate and prove that with automated load testing: For each replication chain we single out a production host, and increase the hosts weight in the load balancer until the system load1 becomes critical.</description>
    </item>
    
    <item>
      <title>MySQL: Ecosystem fragmentation</title>
      <link>/2020/10/28/mysql-ecosystem-fragmentation.html</link>
      <pubDate>Wed, 28 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/28/mysql-ecosystem-fragmentation.html</guid>
      <description>Sometimes things change in a way that is hard to put a finger on, but I am doing this MySQL thing since 3.23, and commercially since 2005, and the environment is changing. These days, when you talk to people in need of MySQL, the first thing you have to ask them is &amp;ldquo;Which MySQL&amp;rdquo;. And by that I do not mean a version number in the first place.
The answer may be:</description>
    </item>
    
    <item>
      <title>An unexpected pool size increase</title>
      <link>/2020/10/07/an-unexpeced-pool-size-increase.html</link>
      <pubDate>Wed, 07 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/07/an-unexpeced-pool-size-increase.html</guid>
      <description>At work, replication chains have a single primary database node, to which you write, and then multiple replicas, in multiple AZs.
Here is what the one sample chain looks like in Orchestrator:
instance-918d is the current primary, in the blue AZ. Replicas in orange and green are in other AZs. Blue badges indicate multiple replicas, eg (38) means 38 machines.
When you talk to a database, you get two database handles:</description>
    </item>
    
    <item>
      <title>What are the problems with POSIX?</title>
      <link>/2020/10/05/what-are-the-problems-with-posix.html</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/10/05/what-are-the-problems-with-posix.html</guid>
      <description>Every once in a while there is the IT news article that kind of triggers me. This time it was &amp;ldquo;Object-Storage-Protokoll könnte Posix ablösen&amp;rdquo; in german computer news site Golem . The article speaks about mmap(), NVMEoF and object storage and how it could revolutionize or complete object storages, but does not link to an original article, names no persons and no paper. Also, what do these things - mmap, NVMEoF, object storage and Posix, even have in common?</description>
    </item>
    
    <item>
      <title>MySQL: Import CSV, not using LOAD DATA</title>
      <link>/2020/09/28/mysql-import-csv-not-using-load-data.html</link>
      <pubDate>Mon, 28 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/28/mysql-import-csv-not-using-load-data.html</guid>
      <description>All over the Internet people are having trouble getting LOAD DATA and LOAD DATA LOCAL to work. Frankly, do not use them, and especially not the LOCAL variant. They are insecure, and even if you get them to work, they are limited and unlikely to do what you want. Write a small data load program as shown below.
Not using LOAD DATA LOCAL   The fine manual says :</description>
    </item>
    
    <item>
      <title>Importing account statements and building a data warehouse</title>
      <link>/2020/09/26/my-private-data-warehouse.html</link>
      <pubDate>Sat, 26 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/26/my-private-data-warehouse.html</guid>
      <description>This is an update and translation of a [much older article]({% link _posts/2006-07-23-mein-privates-datawarehouse-sparen-mit-mysql.md %}), which I wrote in German Language back then. I was experimenting with importing the account statements from my German Sparkasse, which at that time were being made available as a CSV.
The initial data load   The data looked like this:
$ head -2 /home/kris/Documents/banking/umsatz-22758031-29122004.csv &amp;quot;Local Account&amp;quot;;&amp;quot;Book Date&amp;quot;;&amp;quot;Valuta Date&amp;quot;;&amp;quot;Transaction Type&amp;quot;; &amp;quot;Purpose&amp;quot;; &amp;quot;Remote Party&amp;quot;;&amp;quot;Remote Account&amp;quot;;&amp;quot;Bank Code&amp;quot;; &amp;quot;Amount&amp;quot;;&amp;quot;Currency&amp;quot;;&amp;quot;Info&amp;quot; &amp;quot;08154711&amp;quot;;&amp;quot;30.</description>
    </item>
    
    <item>
      <title>MySQL: automatic partitions surely would be nice</title>
      <link>/2020/09/25/mysql-dynamic-partitions-suck.html</link>
      <pubDate>Fri, 25 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/25/mysql-dynamic-partitions-suck.html</guid>
      <description>In [Deleting data]({% link _posts/2020-09-24-mysql-deleting-data.md %}) we have been looking at a process that loads data into MySQL, leveraging partitions to make it easier and faster to later get rid of the data again. For this, we created three processes, a data loader process, and two observers - one for creating partitions, and one for deleting them.
The observer processes have been running ANALYZE TABLES and then polling INFORMATION_SCHEMA.PARTITIONS every 1/10th of a second to check if intervention is needed.</description>
    </item>
    
    <item>
      <title>MySQL: Deleting data</title>
      <link>/2020/09/24/mysql-deleting-data.html</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/24/mysql-deleting-data.html</guid>
      <description>Completing the data lifecycle is often harder than originally expected: Deleting data can cost sometimes way more than inserting it in the first place. MySQL Partitions can offer a way out. We have an [earlier post]({% link _posts/2020-05-13-deleting-data-from-mysql.md %}) on the subject.
A sample table, and a problem statement   Let&amp;rsquo;s define a kind of log table, to which data is added with an auto_increment id value and some data.</description>
    </item>
    
    <item>
      <title>MySQL: Provisioning .mylogin.cnf</title>
      <link>/2020/09/23/mylogin-cnf.html</link>
      <pubDate>Wed, 23 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/23/mylogin-cnf.html</guid>
      <description>MySQL uses connection and config parameters from a number of possible sources. The easiest way to find out where it is looking for config files is to run
$ mysql --help | grep cnf order of preference, my.cnf, $MYSQL_TCP_PORT, /etc/my.cnf /etc/mysql/my.cnf /Users/kkoehntopp/homebrew/etc/my.cnf ~/.my.cnf As can be seen, my version of the MySQL client checks in this order
 /etc/my.cnf /etc/mysql/my.cnf /Users/kkoehntopp/homebrew/etc/my/cnf ~/.my.cnf  The cnf file is a file in dot-ini syntax, so you have [groups] and each group contains lines with key = value pairs.</description>
    </item>
    
    <item>
      <title>MySQL: ALTER TABLE for UUID</title>
      <link>/2020/09/22/alter-table-for-uuid.html</link>
      <pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/22/alter-table-for-uuid.html</guid>
      <description>A question to the internal #DBA channel at work: »Is it possible to change a column type from BIGINT to VARCHAR ? Will the numbers be converted into a string version of the number or will be it a byte-wise transition that will screw the values?«
Further asking yielded more information: »The use-case is to have strings, to have UUIDs.«
So we have two questions to answer:
 Is ALTER TABLE t CHANGE COLUMN c lossy?</description>
    </item>
    
    <item>
      <title>MySQL: Encoding fields for great profit.</title>
      <link>/2020/09/18/mysql-encoding-fields-for-great-profit.html</link>
      <pubDate>Fri, 18 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/18/mysql-encoding-fields-for-great-profit.html</guid>
      <description>Iterating schemas over time is not an uncommon thing. Often requirements emerge only after you have data, and then directed action is possible. Consequently, working on existing data, and structuring and cleaning it up is a common task.
In todays example we work with a log table that logged state transitions of things in freeform VARCHAR fields. After some time the log table grew quite sizeable, and the log strings are repeated rather often, contributing to the overall size of the table considerably.</description>
    </item>
    
    <item>
      <title>MySQL from a Developers Perspective</title>
      <link>/2020/09/07/mysql-from-a-developers-perspective.html</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/07/mysql-from-a-developers-perspective.html</guid>
      <description>So this has turned into a small series, explaining how to work with MYSQL from a developers perspective. This post is intended as a directory for the individual articles. It will be amended and re-dated as necessary.
The code for the series is also available in isotopp/mysql-dev-examples on GitHub.
The Tag #mysqldev will reference all articles from this series.
  [MySQL Transactions - the physical side]({% link _posts/2020-07-27-mysql-transactions.md %}). Looking at how MySQL InnoDB handles transactions on the physical media, enabling rollback and commit.</description>
    </item>
    
    <item>
      <title>MySQL: Generated Columns and virtual indexes</title>
      <link>/2020/09/07/mysql-generated-columns-and-virtual-indexes.html</link>
      <pubDate>Mon, 07 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/07/mysql-generated-columns-and-virtual-indexes.html</guid>
      <description>We have had a look at [how MySQL 8 handles JSON]({% link _posts/2020-09-04-mysql-json-data-type.md %}) recently, but with all those JSON functions and expressions it is clear that many JSON accesses cannot be fast. To grab data from a JSON column, you will use a lot of $-&amp;gt;&amp;gt;field expressions and similar, and without indexes nothing of this will be fast.
JSON cannot be indexed.
But MySQL 8 offers another feature that comes in handy: Generated columns and indexes on those.</description>
    </item>
    
    <item>
      <title>MySQL: Basic usage of the JSON data type</title>
      <link>/2020/09/04/mysql-json-data-type.html</link>
      <pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/04/mysql-json-data-type.html</guid>
      <description>MySQL 8 provides solid support for the JSON data type. The manual has an overview of the data type , a JSON function reference , an an overview on generated column indexes , and explains multi-values indexes .
Creating JSON columns   Creating JSON columns is easy: Make the column of the JSON data type, fill in valid JSON data.
mysql&amp;gt; create table t ( id integer not null primary key auto_increment, j json); Query OK, 0 rows affected (0.</description>
    </item>
    
    <item>
      <title>MySQL: Some Character Set Basics</title>
      <link>/2020/08/18/mysql-character-sets.html</link>
      <pubDate>Tue, 18 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/18/mysql-character-sets.html</guid>
      <description>This is the updated and english version of some older posts of mine in German. It is likely still incomplete, and will need information added to match current MySQL, but hopefully it is already useful.
Old source articles in German: 1 , 2 and 3 .
Some vocabulary   Symbol, Font, Encoding and Collation - what do they even mean?
A character set is a collection of symbols that belong together.</description>
    </item>
    
    <item>
      <title>MySQL Foreign Key Constraints and Locking</title>
      <link>/2020/08/04/mysql-foreign-key-constraints-and-locking.html</link>
      <pubDate>Tue, 04 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/04/mysql-foreign-key-constraints-and-locking.html</guid>
      <description>Since we now know how to look at the state of locking in a live database, let&amp;rsquo;s look at what happens when we run a normal insert or update and an insert or update with foreign key relationships defined, and compare.
We will be using the tables and structures from our previous examples, a simple 1:n relationship between a and b:
CREATE TABLE a ( a_id int NOT NULL AUTO_INCREMENT, PRIMARY KEY (a_id) ); INSERT INTO a VALUES (10), (20), (30), (40); CREATE TABLE b ( b_id int NOT NULL AUTO_INCREMENT, a_id int NOT NULL, PRIMARY KEY (b_id), KEY `a_id` (a_id), CONSTRAINT a_id_exists FOREIGN KEY (a_id) REFERENCES a (a_id) ON DELETE RESTRICT ON UPDATE RESTRICT ); INSERT INTO b VALUES (10,10), (40,40); or the same definition for b without the constraint.</description>
    </item>
    
    <item>
      <title>MySQL Foreign Keys and Foreign Key Constraints</title>
      <link>/2020/08/03/mysql-foreign-keys-and-foreign-key-constraints.html</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/03/mysql-foreign-keys-and-foreign-key-constraints.html</guid>
      <description>Foreign Keys are what links tables together and turns a set of tables into a model. Foreign Key Constraints are conditions that must be true for the content of the tables to be an internally consistent model. Foreign Key Constraints can be defined and enforced in InnoDB, but this comes at a considerable price, and for some it may hurt more than it is worth.
A very simple shop as a ER-model.</description>
    </item>
    
    <item>
      <title>MySQL Deadlocks with INSERT</title>
      <link>/2020/08/02/mysql-deadlocks-with-insert.html</link>
      <pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/02/mysql-deadlocks-with-insert.html</guid>
      <description>Support Channel. &amp;ldquo;Hi, I am getting deadlocks in the database and they occur when I have to rollback the transactions but if we don&amp;rsquo;t have to roll back all transactions get executed.&amp;rdquo; Wait, what? After some back and forth it becomes clear that the Dev experiences deadlocks and has data:
mysql&amp;gt; pager less mysql&amp;gt; show engine innodb status\G ... MySQL thread id 142531, OS thread handle 139990258222848, query id 4799571 somehost.</description>
    </item>
    
    <item>
      <title>MySQL: Locks and Deadlocks</title>
      <link>/2020/08/01/mysql-locks-and-deadlocks.html</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/01/mysql-locks-and-deadlocks.html</guid>
      <description>In a [previous article]({% link _posts/2020-07-30-mysql-transactions&amp;mdash;writing-data.md %}) we wrote data to the database using atomic update statements, and then using transactions with SELECT ... FOR UPDATE. In this article we will look at what happens when we continue doing this, in a more complicated way. Source code for this article is also available on github.com .
A simple row lock   But first let&amp;rsquo;s do things manually: We create a table kris with an integer primary key column and a secondary unindexed data column.</description>
    </item>
    
    <item>
      <title>MySQL Transactions - writing data</title>
      <link>/2020/07/30/mysql-transactions-writing-data.html</link>
      <pubDate>Thu, 30 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/30/mysql-transactions-writing-data.html</guid>
      <description>Using the framework for testing we created in earlier articles, let&amp;rsquo;s try to modify some data. We are writing a small program that increments a counter. Our table looks like this, and contains 10 counters:
CREATE TABLE `demo` ( `id` bigint unsigned NOT NULL AUTO_INCREMENT, `counter` int NOT NULL DEFAULT &amp;#39;0&amp;#39;, UNIQUE KEY `id` (`id`) ) INSERT INTO `demo` VALUES (1,0); INSERT INTO `demo` VALUES (2,0); ... INSERT INTO `demo` VALUES (10,0); We are using some very simple programming to increment a counter:</description>
    </item>
    
    <item>
      <title>MySQL Transactions - the logical side</title>
      <link>/2020/07/29/mysql-transactions-the-logical-view.html</link>
      <pubDate>Wed, 29 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/29/mysql-transactions-the-logical-view.html</guid>
      <description>After having a look [how MySQL handles transactions physically]({% link _posts/2020-07-27-mysql-transactions.md %}), let&amp;rsquo;s have a look at what is going on from a logical point of view.
We are using a test table called demo with an id and a counter field, both integer. In it we have 10 counters, all set to 0.
CREATE TABLE `demo` ( `id` bigint unsigned NOT NULL AUTO_INCREMENT, `counter` int NOT NULL DEFAULT &amp;#39;0&amp;#39;, UNIQUE KEY `id` (`id`) ) INSERT INTO `demo` VALUES (1,0); INSERT INTO `demo` VALUES (2,0); .</description>
    </item>
    
    <item>
      <title>MySQL Connection Scoped State</title>
      <link>/2020/07/28/mysql-connection-scoped-state.html</link>
      <pubDate>Tue, 28 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/28/mysql-connection-scoped-state.html</guid>
      <description>MySQL speaks its own proprietary protocol. It cannot be routed by a HTTP proxy, and a MySQL connection is entire unlike a HTTP connection. Specifically, a lot of state and configuration is tied to a MySQL connection, and it cannot be recovered on disconnect.
What state is tied to a connection?   Transactions   A disconnect implies a ROLLBACK. So if you are in a transaction, all changes to the database that you attempted are lost, rolled back, as if they never happened.</description>
    </item>
    
    <item>
      <title>MySQL Commit Size and Speed</title>
      <link>/2020/07/27/mysql-commit-size-and-speed.html</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/27/mysql-commit-size-and-speed.html</guid>
      <description>When writing data to disk, for small transactions the cost of writing the commit out do disk dominates the execution time of the script. In order to show that, I wrote a little bit of Python.
The script creates a test table in a database and writes 10.000 rows of test data into it, in commit sizes of 1, 2, 4, &amp;hellip;, 1024 rows.
$ ./mysql.py --help Usage: mysql.py [OPTIONS] COMMAND [ARGS].</description>
    </item>
    
    <item>
      <title>MySQL Transactions - the physical side</title>
      <link>/2020/07/27/mysql-transactions.html</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/27/mysql-transactions.html</guid>
      <description>So you talk to a database, doing transactions. What happens actually, behind the scenes? Let’s have a look.
There is a test table and we write data into it inside a transaction:
CREATE TABLE t ( id serial, data varbinary(255) ) START TRANSACTION READ WRITE INSERT INTO t ( id, data ) VALUES (NULL, RANDOM_BYTES(255)) COMMIT The MySQL test instance we are talking to is running on a Linux machine, and otherwise idle to make observation easier.</description>
    </item>
    
    <item>
      <title>Where do the JOINs go?</title>
      <link>/2020/06/10/where-do-the-joins-go.html</link>
      <pubDate>Wed, 10 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/10/where-do-the-joins-go.html</guid>
      <description>I was asking on Twitter:
 Are you a Developer and understand (Micro-) Services? I am a database person and a bit simple, and I have a genuine Question:
When moving to a services architecture, where do the JOINs go?
I gave the following context:
 A simple shop
So you sell stuff, that is, you have an orders table o with an oid, which stores a customer id cid from a customers c table, and an article id aid, from an articles table a and a count cnt.</description>
    </item>
    
    <item>
      <title>Deleting data from MySQL</title>
      <link>/2020/05/13/deleting-data-from-mysql.html</link>
      <pubDate>Wed, 13 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/05/13/deleting-data-from-mysql.html</guid>
      <description>I have been pointed at the following question: »Has anyone ever used mySQL events to auto-delete rows from a table after set period? Wondering your experience of doing this.«
 There are two ends to this question:
 expiring data from a MySQL table doing this with the event scheduler  Mass-deleting data from InnoDB   You can of course delete data from a table using the SQL DELETE statement with an arbitrary WHERE-clause at any time:</description>
    </item>
    
    <item>
      <title>Data Access Programs and Pre-SQL systems</title>
      <link>/2020/05/01/data-access-programs.html</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/05/01/data-access-programs.html</guid>
      <description>Let&amp;rsquo;s have a look at very old database systems. Things like dBase or compatible systems (&amp;ldquo;xBases&amp;rdquo;) allowed to manipulate record based files (with indexes) in a procedural way. What does that mean?
xBase   The system allows developers to create files that contain a well defined (fixed size) record structure, basically an array of struct on disk, often together with the definition of on-screen masks designed to show a single record and a (searchable) list of records.</description>
    </item>
    
    <item>
      <title>Some latency numbers illustrated</title>
      <link>/2020/02/28/some-latency-numbers-illustrated.html</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/28/some-latency-numbers-illustrated.html</guid>
      <description>These images are older than time itself. I picked them up while working as a consultant for MySQL AB, but I do not know the source.
Here are some important latency numbers. A pixel is a nanosecond (nano = 10^-9, a billionth of a second, 1 billion events/s = 1 GHz):
And below is the HDD disk seek latency in full at the same scale. An uncached index access can result in up to 5 disk seeks, worst case.</description>
    </item>
    
    <item>
      <title>MySQL Does Disk I/O</title>
      <link>/2019/09/27/mysql-does-disk-io.html</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/09/27/mysql-does-disk-io.html</guid>
      <description>We had a discussion at work about MySQL doing Disk I/O to a NVME disk, and if it is valid to turn off the doublewrite buffer when using XFS.
TL;DR: It&amp;rsquo;s not, you can turn off the doublewrite buffer only on filesystems that never do in-place updates (ZFS, btrfs), or that have their own doublewrite buffer (ext4 with journal=data). A flash layer underneath the actual filesystem is likely not going to help you without additional measures.</description>
    </item>
    
    <item>
      <title>On cache problems, and what they mean for the future</title>
      <link>/2017/06/23/on-cache-problems-and-what-they-mean-for-the-future.html</link>
      <pubDate>Fri, 23 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/23/on-cache-problems-and-what-they-mean-for-the-future.html</guid>
      <description>This is a disk utilization graph on a heavily loaded Graphite box. In this case, a Dell with a MegaRAID, but that actually does not matter too much.
Go-carbon was lagging and buffering on the box, because the SSD was running at its IOPS limit. At 18:10, the write-back cache and the &amp;ldquo;intelligent read-ahead&amp;rdquo; are being disabled, that is, the MegaRAID is being force-dumbed down to a regular non-smart controller. The effect is stunning.</description>
    </item>
    
  </channel>
</rss>
